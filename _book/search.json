[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Statistics 25/26",
    "section": "",
    "text": "Welcome to Advanced Statistics\nThis page contains the core materials for the module. Each week, we will typically complete one tutorial and one worksheet. If there are slides associated with the workshop, these will be to the respective on Canvas &gt; Advanced Statistics &gt; Units &gt; Week X.",
    "crumbs": [
      "Welcome to Advanced Statistics"
    ]
  },
  {
    "objectID": "index.html#using-posit-cloud",
    "href": "index.html#using-posit-cloud",
    "title": "Advanced Statistics 25/26",
    "section": "Using Posit Cloud",
    "text": "Using Posit Cloud\nYou can find the information on joining the Advanced Statistics Posit Cloud workspace on Canvas by going to Module Information &gt; Module materials .\nEach new project you create will have two folders:\n\nquarto: containing a file for making notes and a file for completing the worksheet\ndata: empty by default. For each tutorial, you will need to download the data from this page and upload it into this folder.\n\n\nDownloading and uploading datasets\nEach tutorial contains a link to the dataset under The data section. It might look something like this:\n\n… the data for this scenario are stored in the file climbing_data.csv\n\nTo download the data:\n\nRight-click on the hyperlink (or tap with two fingers)\nFrom the pop-up menu, select “Save as” or an equivalent, like “Save linked file as” or “Save link as”\nYour browser will then either ask you to choose which folder you want to store the file in, or it will go automatically into downloads.\n\nTo upload the data to Posit:\n\nOpen a project on Posit\nNavigate to the data folder under “Files” (bottom right pane)\nSelect “Upload”. Navigate to the folder with the downloaded file and select it.\n\nOnce the data are in the correct folder, the code from the tutorial will work without any alterations.",
    "crumbs": [
      "Welcome to Advanced Statistics"
    ]
  },
  {
    "objectID": "index.html#tutorials-and-suggested-workflow",
    "href": "index.html#tutorials-and-suggested-workflow",
    "title": "Advanced Statistics 25/26",
    "section": "Tutorials and suggested workflow",
    "text": "Tutorials and suggested workflow\nDuring the first half of the workshop, we will work through the main parts of the tutorial together - I will demonstrate how to approach tasks and highlight what things to look out for. The second part of the workshop is dedicated to independent work, in which you’re expected to complete the following exercises:\n\nThe What does this code do? section\nThe Worksheet\n\nboth of which will be based on the tutorial we’ve just covered. How exactly you use these tutorials will depend on how you prefer to learn. Here are some options:\nOption 1: Prepare before the session\n\nWork through the tutorial before the session, trying out different bits of code in the Quarto file on Posit Cloud\nListen during the demonstration to check your your understanding, perhaps make some additional notes.\nJump straight into working on the independent exercises. Ask if you’re not sure about anything.\n\nOption 2: Learn as you go\n\nTreat the workshops as your first point of contact with the new material.\nListen during the demonstration to take in new information.\nDuring the independent work, revisit any parts of the tutorial which were unclear during the demonstration. Then use the tutorial to complete the worksheet. Ask if you’re not sure about anything.\n\nOption 3: The middle ground\n\nBefore the workshop, prepare a quarto file that contains all the code from the tutorial for that week - you don’t necessarily need to read the whole tutorial to do this.\nDuring the demonstration, listen, run the code along with the tutor and make notes.\nJump straight into working on the independent exercises. Revisit any part of the tutorial that were unclear during the demonstration. Ask if you’re not sure about anything.\n\nOr any combination of the three. The point is, you can either use the tutorials as a reference point or as a way to prepare for the session - whatever you choose to do is up to you, as long as you’re able to complete the independent exercises each week.\nCan I complete the exercises at home?\nYes, there’s nothing stopping you from doing so. The main benefit of working from them in the session is that you’ll be able to talk to your peers and get feedback directly from the tutor.",
    "crumbs": [
      "Welcome to Advanced Statistics"
    ]
  },
  {
    "objectID": "01a_gzlms_skewed.html",
    "href": "01a_gzlms_skewed.html",
    "title": "1  Modelling skewness",
    "section": "",
    "text": "1.1 Gamma models\nGamma models are used for modelling right skewed distributions. That is, skewed distributions with the tail on the right side. Unlike Gaussian distributions, which are defined by the mean and the standard deviation, Gamma distributions are defined by shape and scale parameters. These parameters define the extent of skewness and the length of tails, but how shape and scale work together to create a distribution is a little less intuitive than mean and SD in a normal distribution. So let’s have a look:\n1gamma_dist &lt;- rgamma(n = 10000, shape = 1, scale = 1)\n\n2ggplot2::ggplot(data = NULL, aes(x = gamma_dist)) +\n3  geom_density()\n\n\n1\n\nGenerate a gamma distribution. We’re generating 1000 data points n = 1000. We’re also setting both the shape and scale to be equal to 1. Save the result to the object called gamma_dist\n\n2\n\nInitialise a ggplot. gamma_dist is just a vector of numbers, not a dataset. So we set the data argument to NULL, and we use gamma_dist on the x axis.\n\n3\n\nAdd a density plot to show the distribution.\nShape and scale of 1 generate a very skewed distribution with a long right tail. How about changing the scale to 2?\ngamma_dist &lt;- rgamma(n = 10000, shape = 1, scale = 2)\n\nggplot2::ggplot(data = NULL, aes(x = gamma_dist)) +\n  geom_density()\nIf we change the scale, the tail becomes even longer and the distribution is more pointy. What if we change scale back to 1 and this time set shape to 2?\ngamma_dist &lt;- rgamma(n = 10000, shape = 2, scale = 1)\n\nggplot2::ggplot(data = NULL, aes(x = gamma_dist)) +\n  geom_density()\nThe mass of the distribution has slightly shifted to the right. There is a lot of flexibility in the kinds of distributions that can be modelled under the Gamma family, which makes it incredibly useful and broadly applicable.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelling skewness</span>"
    ]
  },
  {
    "objectID": "01a_gzlms_skewed.html#gamma-models",
    "href": "01a_gzlms_skewed.html#gamma-models",
    "title": "1  Modelling skewness",
    "section": "",
    "text": "Data visualisation\nWe’re going to recreate Figure 1, but this time, we’re also adding the prediction line for a would-be Gamma model. We do so by including the following line in the ggplot”\n\n... + \nstat_smooth(method = \"glm\", colour = \"darkorange\", fill = \"darkorange\", \n            method.args = list(family = \"Gamma\")) + \n...\n\n\nmethod = \"glm\": we change this from “lm” to indicate that we’re fitting a generalised linear model, not a general linear model.\ncolour = \"darkorange\", fill = \"darkorange\": these are the same as before, we’re just changing the colour code\nlinetype = \"dashed\": optional argument to differentiate the type of the line. This is only needed if we’re showing multiple lines on the plot.\nmethod.args = list(family = \"Gamma\") - these are the arguments that we want to specify for the method. We can use multiple arguments here, so we wrap everything in the list function. Inside the list function, we specify that the family we’re using to construct the line is \"Gamma\".\n\n\nbelay_tib |&gt; \n  ggplot2::ggplot(data = _, aes(x = climbing_exp, y = reaction_time)) + \n  geom_point(alpha = 0.7, colour = \"yellowgreen\") + \n  stat_smooth(method = \"lm\", colour = \"steelblue\", fill = \"steelblue\") + \n  stat_smooth(method = \"glm\", \n              colour = \"darkorange\", fill = \"darkorange\", \n              linetype = \"dashed\",\n              method.args = list(family = \"Gamma\")) + \n  coord_cartesian(xlim = c(0, 10), ylim = c(0, 1030.5)) + \n  labs(x = \"Belayer's climbing experience (in years)\", y = \"Reaction time (ms) to break the fall\") + \n  theme_light() \n\n\n\n\n\n\n\nFigure 1.1\n\n\n\n\n\nWe’re keeping the original straight line on the plot just for comparison but though we wouldn’t usually do this if we’re just fitting a Gamma model.\nTwo things to note here:\n\nPrediction: The dashed line (the Gamma model) is better able to predict the cases on the left end of the distribution by tilting upwards. Likewise, the line becomes less steep towards the right end, suggesting it won’t go on downwards forever.\nNon-linear relationship: The line is not straight - the rate of change (the slope) is different in different sections of the plot. This could be a problem for a General Linear Model, but Generalised Linear Models have an in-built way way of modelling this.\n\n\n\nLink functions\nGaussian models assume that the conditional distribution of the outcome is normal. Normal distribution is unbound - it ranges from \\(-\\infty\\) to \\(+\\infty\\) , which makes it possible and convenient to fit straight lines that also go from \\(-\\infty\\) to \\(+\\infty\\) (see section Going Beyond the data). The slope of this line is fixed, which means that we’re able to describe the relationship between two variables with just one parameter estimate. The problem is that many distributions we encounter in real life are bound on one end or both and might not be symmetrical. For example the Gamma distribution can only ever contain positive values (i.e. values greater than 0), so fitting a line that extends to infinity just won’t do. But we still want a single parameter with which to test our hypothesis.\nEnter link functions. They are functions that we apply to the conditional outcome distribution so that we can express the relationship between the outcome and the predictor as a linear term, such that:\n\\[\ng\\{E(Y_i)\\} = \\beta_0 + \\beta_1X_i\n\\]\nHere, the expected1 conditional value of the outcome \\(E(Y_i)\\) at given value of the predictor X is transformed by some (for now unspecified) function \\(g\\) . Because of this transformation, a linear combination of the intercept \\(\\beta_0\\) , the slope \\(\\beta_1\\) and the value of the predictor \\(X\\) can be used to make predictions.\n\n\n\n\n\n\nWatch out! We are not transforming the outcome.\n\n\n\n\n\nSometimes researchers will attempt to transform the raw outcome variable in an attempt to “fix” skewness and then fit a regular GLM (using OLS estimation). In effect, they move the transformation to an earlier step:\n\\[\nE(g(Y_i)) = \\beta_0 + \\beta_1X_i\n\\]\nso that the expected value of the transformed outcome \\(Y_i\\) can be predicted from the linear term. This is a bad idea an not what we’re doing when we’re applying a link function. The link function is applied to the expected values, not to the raw variable. An OLS model fitted to a transformed variable will not only give inaccurate parameter estimates, but the estimates themselves will be uninterpretable with reference to the variable we originally collected.\nThe stats behind GzLM are complicated. Conditional distributions are unintuitive to think about and link functions can be confusing. It’s normal to find this stuff difficult. If you only remember one thing from this whole section, then please let it be this:\nGeneralised Linear Models need to apply a transformation somewhere in the process under the hood to make non-linear prediction possible. This is not the same as transforming the raw variables and then fitting a simple linear model.\n\n\n\n\nCanonical link functions\nDifferent distributions are associated with different link functions that will achieve the linearising transformation described above. We call them “canonical link functions”. We’ll introduce relevant link functions as we learn about different models. For Gamma models, the link function that we need to apply is the logarithm, or the “log” function.\n\n\n\nFit the model\nLet’s fit our model using the glm function, which we introduced in the section Fitting GzLM in R.\n\nbelay_gamma &lt;- glm(\n  reaction_time ~ assisted_break + climbing_exp + distracted + fall_m,\n  family = Gamma(link = \"log\"), \n  data = belay_tib \n)\n\nThe part of the code that does all the heavy lifting is this: family = Gamma(link = \"log\") in which we’re asking the function to use the Gamma family and apply the log link function.\n\n\nCheck model assumptions\nWe can use the same function as before to check for model assumptions. For Gamma models, the output is going to be very similar, but the underlying calculations differ in some instances.\n\nbelay_gamma |&gt; check_model()\n\n\n\n\n\n\n\nFigure 1.2\n\n\n\n\n\n\nPosterior predictive checks - this is an obvious indicator that the Gamma distribution was a good choice. The simulated predicted values match the observed data much better than they did for an OLS model (Figure 1).\nHomogeneity of variance - this plot can be interpreted in the same way as the plot for OLS models. That is, we’re hoping to not see any patterns or curvature, which appears to be the case. The residuals we see plotted, however, are so called “Pearson residuals”, which can be thought of as the differences between the predicted and the observed values adjusted for the variance - the function for variance will differ depending on the distribution we’re working with.\nInfluential observations - there don’t appear to be any influential observations based on the Cook’s distances or Leverage values. Like the previous plot, this plot also shows the Pearson residuals.\nCollinearity - All VIF values are below 10, suggesting no issues with multi-collinearity.\nUniformity of residuals - obviously, we’re not expecting the residuals to be normally distributed, so that’s not what we’re checking here. What we are checking is whether the residuals follow a uniform distribution - if the the dots are following the diagonal line closely, the residuals are uniformly distributed. The residuals on this plot are, again, not the differences between the predicted and the observed values, nor they are Pearson residuals. They are simulated residuals based on making repeated predictions from our model (similar to bootstrapping)\n\nThese assumptions are not assumptions in the same sense as they are for OLS models. In OLS models, if normality or heteroscedasticy are violated, we cannot trust the p-values and confidence intervals because of the maths underlying the standard errors. For GzLM, the underlying logic is a little different. Gamma models do not assume homoscedasticity - quite the opposite. They also don’t assume a uniform distribution for the residuals. But if the specific type of residuals we extracted is uniform, it’s a signal that the model is well specified. All of the model checks above are just that - they check whether the model we have assumed (i.e. the Gamma model) makes sense given the data. For all intents and purposes we do need to check them and take them as seriously as “classic” assumptions, but the reasons for checking are different.\n\n\n\n\n\n\nThe depths of residual hell (optional section that will make you sad)\n\n\n\n\n\nIf the section above reads a bit like “trust me bro”, blame the check_model function (that’s what I do anyway) which hides all of the complicated explanations in exchange for convenience and a fraction of your soul every time you use it. This section explains some of the weirdness with the different types of residuals. You can skip it if you have better things to do. If you don’t have better things to do, I suggest picking up embroidery or climbing.\nStill persisting? Very well. Let’s start with the more complicated ones, which is the uniformity of residuals. Like we said above, GzLM doesn’t assume a uniform distribution of the residuals, and it doesn’t assume normal distribution of the residuals. How about Gamma-distributed residuals for Gamma models? Also no - recall that Gamma distribution can only take on positive values. Residuals represent the error in the model prediction, and this error can be positive or negative so a Gamma distribution is impossible for residuals, unless we’ve really misspecified the model.\nIn a well specified model, roughly about a half of the residuals will be positive a half will be negative (or a half of the points will be above the line and half will be below the line, as in Figure 1.1). This of course doesn’t allow us to place any expectations on the shape of the distribution other than that it will be symmetrical and centred around zero.\nThe check for uniform residuals turbo-charges this idea by running simulations based on our model. R has an in-built simulate function that can be applied to any compatible model to generate predictions. For example:\n\nsimulated_predictions &lt;- simulate(belay_gamma, nsim = 10)\nsimulated_predictions\n\n\n\n\n\n\n\nThis will take our model 10 times and create 10 sets of new predictions for the value of the outcome (reaction time) based on the model we fitted. Were are the values being draw from? We we request a standard non-easystats-y summary of our model we get this:\n\nbelay_gamma |&gt; \n  summary()\n\n\nCall:\nglm(formula = reaction_time ~ assisted_break + climbing_exp + \n    distracted + fall_m, family = Gamma(link = \"log\"), data = belay_tib)\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     6.62505    0.09243  71.674  &lt; 2e-16 ***\nassisted_break -0.06913    0.03094  -2.234 0.026965 *  \nclimbing_exp   -0.11136    0.01052 -10.591  &lt; 2e-16 ***\ndistracted      0.13151    0.03063   4.294 3.18e-05 ***\nfall_m          0.14996    0.03781   3.966 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.03501732)\n\n    Null deviance: 9.9564  on 151  degrees of freedom\nResidual deviance: 4.7078  on 147  degrees of freedom\nAIC: 1809\n\nNumber of Fisher Scoring iterations: 4\n\n\nSpecifically of interest here is this:\nDispersion parameter for Gamma family taken to be 0.03501732\nSo the simulation takes the first row of our dataset, considers the values of the predictors and builds a Gamma distribution conditional on the predictors using the “dispersion parameter”. The dispersion parameter is directly related to the scale parameter \\(\\theta\\). Now that we have this distribution, it randomly selects as many predicted values as we requested. In the example above, we set nsim = 10 so it selects 10 of them.\nOnce we have these 10 values, it compares each of them to the actual value of the outcome - again, this will come form our original dataset. The outcome will either be smaller or larger than each predicted value. Let’s stick with the first row:\n\nbelay_tib[1, ] |&gt; \n  display()\n\n\n\n\n\n\n\n\n\n\n\n\nsubj_id\nreaction_time\nassisted_break\ndistracted\nclimbing_exp\nfall_m\n\n\n\n\n1017\n847.40\n0\n1\n3.10\n2.10\n\n\n\n\n\nThe reaction time for the first row is 847.4. Now let’s look at the values simulated for this first row:\n\nsimulated_predictions[1, ] |&gt; \n  display()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsim_1\nsim_2\nsim_3\nsim_4\nsim_5\nsim_6\nsim_7\nsim_8\nsim_9\nsim_10\n\n\n\n\n570.40\n919.90\n896.34\n585.22\n671.52\n817.29\n845.98\n535.01\n694.31\n736.88\n\n\n\n\n\nThe outcome value is smaller than 2 of these predictions. If we want this as a proportion, we divide this by the number of simulations, which is 10 and get 0.2 .\nWe repeat this process for all of the rows. So next we create a distribution conditional on the values of the predictors in the 2nd row of belay_tib and then draw from it, then the 3rd row, 4th row, and so on until we’ve done this for all the rows. The function also draws more than 10 values - the default is 250. At the end of this process, we’ll have a proportion of cases in which the outcome is smaller and larger than the simulated values for each row. So in our case, we’ll have a distribution of 152 proportions. This distribution is then checked against a uniform distribution. If it’s a good enough match, we conclude the model is well specified. I’ve included some code below that demonstrates the whole process to the point of obtaining the final distribution. It’s not annotated, but if you’ve come this far you can have a go at unpicking what each line does.\n\nnsim &lt;-  250\nsim_data &lt;- simulate(belay_gamma, nsim = nsim)\n\nsim_data &lt;- sim_data |&gt; \n  dplyr::mutate(\n    actual_outcome = belay_tib$reaction_time\n  )\n\nsim_data &lt;- sim_data |&gt; \n  dplyr::mutate(\n    dplyr::across(\n      .cols = -actual_outcome,\n      .fns = \\(x) actual_outcome &lt; x\n    )\n  )\n\ncounts &lt;- sim_data |&gt; \n  dplyr::select(-actual_outcome) |&gt; \n  rowSums(na.rm = TRUE)\n\nproporions &lt;- counts / nsim\n\nggplot2::ggplot(data = NULL, aes(x = proporions)) + \n  geom_histogram(fill = \"yellowgreen\", colour = \"yellowgreen\", alpha = 0.5) + \n  theme_light()\n\n\n\n\n\n\n\n\nThe distribution above is the one compared against a uniform distribution.\nNow onto the issue of heteroscedasticity and Pearson residuals.\nYou may recall that we talked about how GzLM can model different types of variance between the predicted values and the variance (in the Mean-variance relationship section), including heteroscedastic ones. For Gamma models, this relationship is quadratic:\n\\[\nV(Y) = E(Y)^2\n\\]\nThat is, for each predicted value, the variance is expected to be that value squared. This is clearly the opposite of homoscedasticity so why then are we looking for it in Figure 1.2? That’s because we don’t expect to see heteroscedasticity in Pearson residuals.\nRaw residuals - the one we’re used to talking about in OLS models - are calculated as the difference between the actual value and the predicted value of the outcome:\n\\[\n\\epsilon^{raw}_i = y_i - \\hat\\mu_i\n\\]\nin which \\(\\hat\\mu_i\\) is the predicted value. We get extract the raw residuals out of our model if we so wish:\n\nfitted_values &lt;- belay_gamma$fitted.values\nactual_values &lt;- belay_tib$reaction_time\nraw_residuals &lt;- fitted_values - actual_values\n\nNow let’s plot them against the fitted values to get the “classic” diagnostic plot:\n\nggplot2::ggplot(data = NULL, aes(x = fitted_values, y = raw_residuals)) + \n  geom_point() + \n  theme_light()\n\n\n\n\n\n\n\n\nWould you look at that! The shape is funneling out, the heteroscedasticity is still there. But checking this plot as a way of diagnosing the Gamma model would tell us nothing of use because the model doesn’t assume the absence of heteroscedasticity.\nSo we look at the Pearson residuals instead, which are calculated as:\n\\[\n\\epsilon^{pearson}_i = \\frac{y_i - \\hat\\mu_i}{\\sqrt{V(\\hat\\mu_i)}}\n\\]\nThe top of the fraction is the same. The bottom part, specifically the \\(V\\) is the function for calculating variance in a given model, which changes depending on the distribution. This will “model away” the heteroscedsticity, which is why we’re still looking for a random non-funnelling shape when checking the diagnostics.\nIf you made it this far, well done. This is about the maximum amount of information I’m willing to know about the topic at this stage of my life. There is no more to be learned by asking me clarifying questions, but you can certainly try.\n\n\n\n\n\nCheck model fit\nAs usual, we want:\n\nThe overall fit statistics\nThe test of model fit\n\nAs previously, easystats has our back and we can just use the familiar functions.\n\nbelay_gamma |&gt; \n  performance() |&gt; \n  display()\n\n\n\nTable 1.1\n\n\n\n\n\n\nAIC\nAICc\nBIC\nNagelkerke’s R2\nRMSE\nSigma\n\n\n\n\n1809.0\n1809.6\n1827.2\n0.54\n100.17\n0.19\n\n\n\n\n\n\n\n\nThe main difference here is that instead of typical R2 we get so called, Nagelkerke’s R2 . The R2 in OLS linear model is based on least squares calculations. In GzLM, the R2 is based on what we call “deviance”, and because of this, we call it “Pseudo-R2” - kind of like R2 but not quite.\nLike R2 and the F statistic, deviance is a goodness-of-fit measure. In plain language, it tells us how much our model differs from a model that is perfectly defined. A perfectly-defined model is sometimes called a saturated model - it is a model that can predict the outcome without any error whatsoever. In the perfectly-defined model, the deviance would be zero - therefore the less deviance we have, the better.\nThe performance output doesn’t provide deviance for us, but we can request it easily by asking R to show us the default ugly model output without any additional code:\n\nbelay_gamma |&gt; summary()\n\n\nCall:\nglm(formula = reaction_time ~ assisted_break + climbing_exp + \n    distracted + fall_m, family = Gamma(link = \"log\"), data = belay_tib)\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     6.62505    0.09243  71.674  &lt; 2e-16 ***\nassisted_break -0.06913    0.03094  -2.234 0.026965 *  \nclimbing_exp   -0.11136    0.01052 -10.591  &lt; 2e-16 ***\ndistracted      0.13151    0.03063   4.294 3.18e-05 ***\nfall_m          0.14996    0.03781   3.966 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.03501732)\n\n    Null deviance: 9.9564  on 151  degrees of freedom\nResidual deviance: 4.7078  on 147  degrees of freedom\nAIC: 1809\n\nNumber of Fisher Scoring iterations: 4\n\n\nAt the very bottom, we can see the following:\n\nNull Deviance: 9.956\nResidual Deviance 4.708\n\nNull deviance (D0) tells us how much deviance there is in a model that doesn’t have any predictors. The residual deviance (DR) is the deviance that remains after we’ve added our predictors. We can then calculate the pseudo R2 as:\n\\[\nR^2_\\mbox{pseudo} = 1 - \\frac{D_R}{D_0}\n\\]\nOf course, once a pseudo R2 like the one above was defined, statisticians started an arms race about minor modifications to the equation which would estimate the deviance in the model more accurately. Nagelkerke’s R2 is one of these modifications and generally a good choice. That’s all I will say about it because it is 8pm and I need to deliver this workshop in about a week’s time. Unlike regular R2 , pseudo-R2 is not interpreted as the proportion of variance explained, but as proportion of deviance explained by the model, although statisticians like to debate whether attempting to calculate R2 for ML based models even makes sense in the first place. We’ll leave it at that and use it.\nBased on the pseudo R2 , our model explains 53.53% of total deviance in outcome.\nFinally, let’s test the model fit:\n\nbelay_gamma |&gt; \n  test_wald() |&gt; \n  display()\n\ncode Only one model was provided, however, at least two are required for\ncode   comparison.\ncode   Fitting a null-model as reference now.\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nF\np\n\n\n\n\nNull model\nglm\n151\n\n\n\n\n\nFull model\nglm\n147\n4\n40.97\n&lt; .001\n\n\n\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\n\nOverall the Wald test indicates the model is good fit, F(4,147) = 40.97, p &lt; .001.\n\n\nParameter estimates\nFinally, let’s extract the parameter estimates:\n\nbelay_gamma |&gt; \n  parameters() |&gt; \n  display()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Prevalence\nSE\n95% CI\nt(147)\np\n\n\n\n\n(Intercept)\n6.63\n0.09\n(6.45, 6.81)\n71.67\n&lt; .001\n\n\nassisted break\n-0.07\n0.03\n(-0.13, -8.61e-03)\n-2.23\n0.025\n\n\nclimbing exp\n-0.11\n0.01\n(-0.13, -0.09)\n-10.59\n&lt; .001\n\n\ndistracted\n0.13\n0.03\n(0.07, 0.19)\n4.29\n&lt; .001\n\n\nfall m\n0.15\n0.04\n(0.08, 0.22)\n3.97\n&lt; .001\n\n\n\n\n\nThese estimates are a little more tricky to interpret than we’re used to, but nothing we can’t handle. Remember how we used the link function which applied the logarithmic transformation to the expected values? Now need to reckon with this fact.\nThe parameter estimates as we see above are on the log scale. For example for assisted break, the model predicts -0.07 change in the log of the outcome with the change of one unit of the predictor. The estimate is negative, so we know that as the predictor increases, the log of the outcome decreases (whereas the opposite is true for distraction and fall height). But the values themselves are not particularly illuminating. Luckily, we can transform the betas in a way that will allow us to interpret them in the original units by exponentiating them. This is because exponentiation is the inverse function to the log.\n\nbelay_gamma_exp &lt;- belay_gamma |&gt; \n  parameters(exponentiate = TRUE) \n \nbelay_gamma_exp |&gt;  \n  display()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nPrevalence Ratio\nSE\n95% CI\nt(147)\np\n\n\n\n\n(Intercept)\n753.74\n69.67\n(630.16, 902.53)\n71.67\n&lt; .001\n\n\nassisted break\n0.93\n0.03\n(0.88, 0.99)\n-2.23\n0.025\n\n\nclimbing exp\n0.89\n9.41e-03\n(0.88, 0.91)\n-10.59\n&lt; .001\n\n\ndistracted\n1.14\n0.03\n(1.07, 1.21)\n4.29\n&lt; .001\n\n\nfall m\n1.16\n0.04\n(1.08, 1.25)\n3.97\n&lt; .001\n\n\n\n\n\nNow the predictors that are negatively associated with the outcome have values smaller than 1, while predictors with positive association have values greater than 1. As we’ve seen in Figure 1.1, the relationship between the predictor and the outcome is not entirely linear so the interpretation of the parameters is not as simple as a change in one unit of the predictor associated with the change in the outcome. Instead, what we have is called the Prevalence Ratio. To make the interpretation of PR easier, we’re going to convert PR into percentage change associated with a change in one unit of the predictor. We do this by subtracting 1 from the estimate and then multiplying it by 100. We’ll do this for both the estimate and the confidence intervals:\n\nbelay_gamma_exp |&gt; \n1  tibble::as_tibble() |&gt;\n2  dplyr::mutate(\n3    perc_change = (Coefficient-1) * 100,\n4    perc_ci_lower = (CI_low-1) * 100,\n5    perc_ci_upper = (CI_high-1) * 100\n  ) |&gt; \n6  dplyr::select(-c(CI, df_error)) |&gt;\n  display() \n\n\n1\n\nTake the exponentiated parameter estimates and pipe them convert them into a tibble so we can make some calculations\n\n2\n\nPipe the tibble into the mutate function.\n\n3\n\nCalculate percentage change perc_change by subtracting each parameter estimate in the Coefficient column from 1 and then multiplying the result by 100.\n\n4\n\nPerform the same calculation for lower confidence interval.\n\n5\n\nperform the same calculation for upper confidence interval.\n\n6\n\nRemove unnecessary columns (optional).\n\n\n\n\n\n\nTable 1.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI_low\nCI_high\nt\np\nperc_change\nperc_ci_lower\nperc_ci_upper\n\n\n\n\n(Intercept)\n753.74\n69.67\n630.16\n902.53\n71.67\n0.00\n75274.48\n62916.45\n90152.53\n\n\nassisted_break\n0.93\n0.03\n0.88\n0.99\n-2.23\n0.03\n-6.68\n-12.16\n-0.86\n\n\nclimbing_exp\n0.89\n9.41e-03\n0.88\n0.91\n-10.59\n3.30e-26\n-10.54\n-12.35\n-8.70\n\n\ndistracted\n1.14\n0.03\n1.07\n1.21\n4.29\n1.76e-05\n14.05\n7.41\n21.11\n\n\nfall_m\n1.16\n0.04\n1.08\n1.25\n3.97\n7.30e-05\n16.18\n7.97\n25.04\n\n\n\n\n\n\n\n\nLet’s take this row by row. The intercept is interpreted as usual - it’s the value of the outcome when all the predictors are 0. For this row, we ignore the perc_change columns we’ve just created.\nAssisted breaking device: We read the perc_change column here. For this predictor, participants who used an assisted breaking device had reaction time lower by -6.68% compared to those who used a regular belay device. Let’s give this a little more context and look at the predictions. Once again, we’re creating a helper prediction tibble with the values for which we want to make predictions. We’re holding all the values constant, apart from assisted_break where we want predicted reaction time for the participants who didn’t use a breaking device (0) as well as those who did (1). If you’re wondering where the other values came from, revisit section Making predictions.\n\nprediction_assisted_break &lt;- tibble::tibble(\n  climbing_exp = 6.14,\n  assisted_break = c(0, 1),\n  distracted = 0,\n  fall_m = 1.82\n)\n\nWe then use the predict function to make a prediction from our model:\n\n1predict(belay_gamma, newdata = prediction_assisted_break) |&gt;\n2  exp()\n\n\n1\n\nGenerate prediction\n\n2\n\nExponentiate from the log scale\n\n\n\n\n       1        2 \n499.8019 466.4167 \n\n\nThe first value is the prediction of the reaction time for those without an assisted breaking device (499.8019 ms) the second value predicts the RT for those with an assisted breaking device (466.4167 ms). As expected, the latter time is shorter. Based on our model, the predicted difference 6.68%. What’s 6.68% of 499.8019?\n\n499.8019 / 100 * 6.68\n\n[1] 33.38677\n\n\nNow if we subtract this difference from the predicted value…\n\n499.8019 - 33.38677\n\n[1] 466.4151\n\n\n… we get a predicted value for the second group (ignoring some rounding error on 3rd+ decimal place). This is why we sometimes call this type of parameter estimate a relative change: because we calculate the percentage change relative to a given value.\nClimbing experience: This is a continuous predictor but if you understood the principle of the previous section, this one is not much different. For each year of climbing experience, the model predicts a -10.54% decrease in reaction time. Based on the confidence intervals, this change could be as large as -12.35% or as small as -8.7%.\nThere’s one final thing we need to highlight about the Gamma models. Revisit Figure 1.1 . Recall that the linear prediction goes into infinity, including unrealistic predictions like negative reaction times. However, the “slope” of the Gamma line seems to be less steep the more we move to the right on climbing experience. This is were relative change really shines.\nSay we want to predict the reaction time for someone with two years of climbing experience:\n\npredict(\n  object = belay_gamma, \n  newdata = tibble::tibble(\n    climbing_exp = 2,\n    assisted_break = 0,\n    distracted = 0,\n    fall_m = 1.82\n  )\n) |&gt; exp()\n\ncode        1 \ncode 792.5492\n\n\nWe’ve put the prediction tibble directly into the function to speed things up a bit. While other predictors are held constant, the model predicts a reaction time of 792.5492 for someone who’s been climbing for 2 years. The estimated change for this predictor was 10.54%, which is:\n\n792.5492 / 100 * 10.54\n\n[1] 83.53469\n\n\nSo now we can calculate:\n\n792.5492 - 83.53469\n\n[1] 709.0145\n\n\nand 876.0839 is the reaction time predicted for someone with three years of experience.\nWhat if we took someone at the upper end of climbing experience, say 8 years?\n\npredict(\n  object = belay_gamma, \n  newdata = tibble::tibble(\n    climbing_exp = 8,\n    assisted_break = 0,\n    distracted = 0,\n    fall_m = 1.82\n  )\n) |&gt; exp()\n\ncode        1 \ncode 406.2939\n\n\nThe predicted reaction time is 406.2939, and 10.54% of this value is:\n\n406.2939 / 100 * 10.54\n\n[1] 42.82338\n\n\n42.82. So the value that we’re subtracting becomes lower and lower the further up we move on climbing experience. This effectively prevents us from falling into absurd predictions. Eventually, the prediction line would plateau and be almost completely flat, which makes sense - there’s a physical limit of how fast someone can react.\n\n\nReport\nWhen reporting any model, we want to strike a balance between reporting all the necessary bits of information and interpreting the parameter estimates in a way that allows us to make conclusions about the hypothesis. Here’s an example of how you could approach but don’t feel beholden to this:\n\nWe fitted a Gamma Generalised Linear Model with the log link function predicting belayers reaction time assisted device use and climbing experience. The covariates in the model were the belayers’ climbing experience and whether they were distracted when belaying their climbing partner.\nBased on the posterior predictive checks and residual diagnostics, the model appeared to be well specified. Overall, the model was a good fit, F(4,147) = 40.97, p &lt; .001, and explained 53.53% of deviance in the outcome, R2 = 0.54 (Nagelkerke’s R2).\nIn line with Hypothesis 1, assisted breaking device was a statistically significant predictor of the time it took belayers to stop their climbers’ fall, b = 0.93, 95% CI [0.88, 0.99], SE = 0.03, t = -2.23, p = 0.025, while other predictors were held constant. Belayers who used an assisted breaking device were -6.68% quicker to stop the fall compared to those who didn’t. 95% confidence intervals indicated this difference could be as large as -12.16 or as small as -0.86. This effect was small translated into a difference of 33.38 ms.\nHypothesis 2 was also supported by a statistically significant effect of climbing experience, b = 0.89, 95% CI [0.88, 0.91], SE = 0.01, t = -10.59, p &lt; .001, while holding other predictors constant. A year of climbing experience was associated with a decrease of -10.54%, 95% CI [-12.35%, -8.7%] in belayers’ reaction time. This effect was especially large for belayers with less climbing experience - for example, the model predicted that a belayer with one year of climbing experience would take 885.91 ms to stop a fall while a belayer with two years of experience would take 792.54 ms - a difference of 93.36 ms, which can make a practical difference to the safety of the climber, especially when falling from a lower height where the likelihood of injury is increased.\nThe effects of both covariates (distraction and fall height) - were also statistically significant - full results are reported in Table 1.2.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelling skewness</span>"
    ]
  },
  {
    "objectID": "01a_gzlms_skewed.html#exercises",
    "href": "01a_gzlms_skewed.html#exercises",
    "title": "1  Modelling skewness",
    "section": "1.2 Exercises",
    "text": "1.2 Exercises\n\nWhat does this code do?\n\n\n\n\n\n\nHere’s all the code we have written in this section. Can you remember what each line of each codechunk does? Are there any codechunks that you struggle to make sense of? Make sure to revisit the section in which it is used and take notes.\n\n\n\n\nlibrary(easystats)\nlibrary(tidyverse)\n\nbelay_tib &lt;- here::here(\"data/climbing_data.csv\") |&gt;\n  readr::read_csv()\n\n\ngamma_dist &lt;- rgamma(n = 10000, shape = 1, scale = 2)\n\nggplot2::ggplot(data = NULL, aes(x = gamma_dist)) +\n  geom_density()\n\n\n... + \nstat_smooth(method = \"glm\", colour = \"darkorange\", fill = \"darkorange\", \n            method.args = list(family = \"Gamma\")) + \n...\n\n\nbelay_tib |&gt; \n  ggplot2::ggplot(data = _, aes(x = climbing_exp, y = reaction_time)) + \n  geom_point(alpha = 0.7, colour = \"yellowgreen\") + \n  stat_smooth(method = \"lm\", colour = \"steelblue\", fill = \"steelblue\") + \n  stat_smooth(method = \"glm\", \n              colour = \"darkorange\", fill = \"darkorange\", \n              linetype = \"dashed\",\n              method.args = list(family = \"Gamma\")) + \n  coord_cartesian(xlim = c(0, 10), ylim = c(0, 1030.5)) + \n  labs(x = \"Belayer's climbing experience (in years)\", y = \"Reaction time (ms) to break the fall\") + \n  theme_light() \n\n\nbelay_gamma &lt;- glm(\n  reaction_time ~ assisted_break + climbing_exp + distracted + fall_m,\n  family = Gamma(link = \"log\"), \n  data = belay_tib \n)\n\n\nbelay_gamma |&gt; check_model()\n\n\nbelay_gamma |&gt; \n  performance() |&gt; \n  display()\n\n\nbelay_gamma |&gt; \n  test_wald() |&gt; \n  display()\n\n\nbelay_gamma |&gt; \n  parameters() |&gt; \n  display()\n\n\nbelay_gamma_exp &lt;- belay_gamma |&gt; \n  parameters(exponentiate = TRUE) \n \nbelay_gamma_exp |&gt;  \n  display()\n\n\nbelay_gamma_exp |&gt; \n  tibble::as_tibble() |&gt; \n  dplyr::mutate( \n    perc_change = (Coefficient-1) * 100, \n    perc_ci_lower = (CI_low-1) * 100, \n    perc_ci_upper = (CI_high-1) * 100 \n  ) |&gt; \n  dplyr::select(-c(CI, df_error)) |&gt; \n  display() \n\n\npredict(\n  object = belay_gamma, \n  newdata = tibble::tibble(\n    climbing_exp = 2,\n    assisted_break = 0,\n    distracted = 0,\n    fall_m = 1.82\n  )\n) |&gt; exp()\n\n\npredict(\n  object = belay_gamma, \n  newdata = tibble::tibble(\n    climbing_exp = 8,\n    assisted_break = 0,\n    distracted = 0,\n    fall_m = 1.82\n  )\n) |&gt; exp()\n\n\n\nWorksheet\nScenario:\n\nThe International Civil Aviation Organisation is reviewing the efficiency of the alerts for the Ground Proximity Warning System (GPWS), which warns pilots when the aircraft is too close to the ground or other terrain (like mountains). Currently, the GPWS alerts the pilots by an auditory signal. The aviation authority is deciding whether to replace the auditory signal with a visual one. Because light moves as at a faster speed than sound, they hypothesise that:\nH1: Pilots will be able to respond significantly faster to visual stimuli, compared to auditory stimuli.\nThey hired you as a cognitive scientist to carry out an experiment and recommend a policy change if appropriate. During the experiment, novice pilots without experience of the existing GPWS were placed in a flight simulator and presented with a warning at a random point during the flight. This warning is either auditory or visual (modality, auditory coded as 0, visual coded as 1) and it is displayed either as a simple signal (a short alarm for the auditory modality or a light indicator for the visual modality), or as a brief message with the instruction “Terrain, pull up!” - either spoken or displayed on a flight monitor (signal_complexity, simple alert coded as 0, message coded as 1). You also measured the pilots’ subjective perception of their own alertness (alertness) ranging from 0 (not alert) to 10 (very alert) and the time from the beginning of the flight to the point when GPWS was triggered in minutes (gpws_time). All of these variables should be included as the predictors in the model. The outcome was measured as the time it took the pilots to respond to the warning with a corrective action in milliseconds (response_time).\n\n\n\n\n\n\n\nUse the tutorial to complete the following tasks:\n\nGenerate descriptive statistics and an informative data visualisation for the hypothesis\nFit two models\n\nModel 1: Assuming a normal conditional distribution for the outcome. Call the model alert_glm .\nModel 2: Assuming a gamma conditional distribution for the outcome. Call the model alert_gamma .\n\nEvaluate the model assumptions and the overall fit for both models. Decide which modelling approach is more appropriate.\nGenerate the parameter estimates for the model you selected in the previous step.\nWrite up a short report (up to 300 words), including:\n\nA summary of your evaluation in Task 3\nOverall fit statistics\nInterpretation of the parameter estimates\nPolicy recommendation based on the results\n\n\n\n\n\nYou can use the “worksheet” file in the quarto folder to prepare the worksheet. The data are stored in the file alert_data.csv. Download the data and import it to Posit Cloud. Once you’ve done so, you can read the data into R by running:\n\nalert_tib &lt;- here::here(\"data/alert_data.csv\") |&gt; readr::read_csv()\n\nRemember to load the necessary packages.\n\n\n\n\n\n\nOptional extra task:\nUse alert_glm and alert_gamma to make two separate response time predictions for a pilot who was presented with a visual alert in form of a short message, and who’s subjective alertness and flight duration before the alert were at the average levels found in the sample.\n\n\n\n\n\nCheck worksheet values\nOnce you’ve finished the worksheet, you can ask me to look through your work and give you feedback. Remember that you should also practice writing up the results in a brief report, not just running the code. If you’re stuck, you can use the quiz below to guide you.\nYou can also use the quiz below - if you fitted the models the correctly, your answers should match the values below.\n\n\n\n\n\n\nWorksheet check\n\n\n\n\n\nHow alert were the pilots in the study on average? 5.14\nWhat was the average time from the start of the flight to the point of the alert? 45.753\nThe Gaussian model predicts an increase of 175.57 milliseconds in response time for the visual modality compared to the auditory modality.\nWhich two assumptions of OLS estimation are the most obviously violated for Gaussian model?\n\n Linearity and influential cases Linearity and heteroscedasticity Normal errors and influential cases Heteroscedasticity and collinearity Heteroscedasticity and normal errors Collinearity and influential cases Collinearity and normal errors\n\nBased on the posterior predictive check, which model is a better fit?\n\n Gamma model Gausian model\n\nThe Gamma model explained 28.6 percent of total deviance in the outcome. Compared to the Gaussian model, this proportion was greater by 3.1 percent.\nOverall, the Gamma model was a good fit, F(4,86) = 8.41, p &lt; .001.\nThe time it took the pilots to respond to visual stimuli was 10.1 percent greater compared to response times to auditory signals. Based on this the proposed change from auditory signals to visual signals…\n\n … should be implemented. … should NOT be implemented\n\nOptional task:\nFor a pilot presented with a visual alert who’s subjective alertness and flight duration before the alert were at average levels, the Gaussian models predict the response time of 1659.69 milliseconds, while the Gamma model predicts a response time of 1647.06 milliseconds.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelling skewness</span>"
    ]
  },
  {
    "objectID": "01a_gzlms_skewed.html#footnotes",
    "href": "01a_gzlms_skewed.html#footnotes",
    "title": "1  Modelling skewness",
    "section": "",
    "text": "You could substitute the term “expected” with “predicted”. The only difference is that we tend to use “predicted” when talking about a single value, and “expected” when talking about a whole distribution, but for all intents and purposes, they are the same.↩︎",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelling skewness</span>"
    ]
  },
  {
    "objectID": "01b_gzlms_counts.html",
    "href": "01b_gzlms_counts.html",
    "title": "2  Modelling counts",
    "section": "",
    "text": "Scenario",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelling counts</span>"
    ]
  },
  {
    "objectID": "01b_gzlms_counts.html#scenario",
    "href": "01b_gzlms_counts.html#scenario",
    "title": "2  Modelling counts",
    "section": "",
    "text": "A university lecturer wishes to understand the predictors of undergraduate student attendance in lectures. They record the number of lectures the students attended (n_lectures) per 11-week term (with one lecture per week). For predictors, they considers:\n\nYear of study - first, second, or third (year)\nHow many lecture recordings they accessed (n_record)\nAverage grade on the module (overall_grade)\n\nThe lecturer wants to include all the predictors in the model. They make a hypothesis regarding the year of study:\nH1: First year students will be more likely to attend lectures compared to second and third year students.\n\n\nPackages\nWe’re using the following packages:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(MASS) # for fitting negative binomial models \nlibrary(pscl) # for fitting zero-inflated models \n\n\n\nThe data\nThe data are stored in the attendance_data.csv. Download the data and import it to Posit Cloud. Once you’ve done so, you can read the data into R by running:\n\nattend_tib &lt;- here::here(\"data/attendance_data.csv\") |&gt; \n  readr::read_csv()\n\nBefore we do anything else, we should always check our data to make sure the reading command worked as intended1\n\nattend_tib \n\n\n\n\n\n\n\n\nThe year variable is a categorical variable with more than two levels. We’re going to convert it into a factor:\n\nattend_tib &lt;- attend_tib |&gt; \n  dplyr::mutate(\n    year = factor(year)\n  )\n\nFor the comparison specified in the hypothesis, we want “Year 1” to be coded as the baseline against which to compare the other two years. We can check the order of our levels with the levels() function:\n\nlevels(attend_tib$year)\n\ncode [1] \"Year 1\" \"Year 2\" \"Year 3\"\n\n\n“Year 1” is listed first, which means that R will use it as the baseline group. We don’t need to do anything else here, but if the groups weren’t in the correct order, we’d need to reorder them with the forecats::fct_relevel() function.\n\n\nDescriptives and data viz\nWe’re using the same code as usual for the descriptives. We’re also adding an argument include_factors = TRUE so that we get some summaries for the categorical variables.\n\ndescribe_distribution(attend_tib, include_factors = TRUE) |&gt; \n  display()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nRange\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nstudent_id\n24937.43\n2884.07\n5132.00\n(20026, 29973)\n0.03\n-1.21\n584\n0\n\n\nn_lectures\n1.33\n1.64\n2.00\n(0, 10)\n1.64\n3.07\n584\n0\n\n\nyear\n\n\n\n(Year 1, Year 3)\n0.18\n2.17\n584\n0\n\n\nn_record\n5.26\n1.63\n2.00\n(0, 11)\n0.08\n0.13\n584\n0\n\n\noverall_grade\n54.58\n13.49\n17.05\n(0, 96.57)\n-0.19\n0.55\n584\n0\n\n\n\n\n\n\nSome things of note - the average number of lectures attended is extremely low - less than two out of 11 lectures. This variable is also quite skewed and has excess kurtosis. Let’s see what it looks like. To spice things up, we’ll create a nice histogram this time, overlayed by a count line. If you’re copying and adjusting code, try copying line by line so you can see the changes gradually:\n\n1ggplot2::ggplot(data = attend_tib, aes(x = n_lectures)) +\n2  geom_histogram(fill = \"yellowgreen\", alpha = 0.4) +\n3  geom_line(stat = \"count\") +\n4  geom_point(size = 2, stat = \"count\") +\n5  scale_x_continuous(breaks = seq(0, 11, 1)) +\n6  theme_light()\n\n\n1\n\nCreate the base layer, placing the number of layers on the x axis\n\n2\n\nAdd the histogram. We’re also changing the default colour and we’re making it a little transparent. This is optional (but looks nice)\n\n3\n\nAdd a line. Set the statistic used for calculating the values to be a “count”.\n\n4\n\nAdd points. We can make them a little larger using the size argument. As above, we’re requesting the “count” statistic.\n\n5\n\nChange the scale to make sure the breaks are only at whole numbers and not at places with decimal points. We could just list all the numbers for the breaks argument - e.g. c(1,2,3,4,5,6,7,8,9,10,11) - but it’s more efficient to request a sequence of numbers starting at 0, going up to 11 (because that’s how many lectures there were in total), increasing by 1 at each step: seq(0, 11, 1) .\n\n6\n\nAdd a nice theme.\n\n\n\n\n\n\n\n\n\n\n\nOften the histogram itself is enough to give us the necessary info. However with count variables, adding the line and the points can be used to show that the outcome variable is indeed a count variable and that we’re going to treat it as such (as opposed to treating it as continuous.\nWe can see that we have a right skewed distribution. Quite a lot of students did not attend any lectures at all or they went to just one. Very few participants attended 10 lectures, and no-one attended all of them (number 11 doesn’t even show up on the plot unless we force the plot to do so).\nFor the hypothesis, we can create a dot plot instead of a scatter plot (because we have a categorical predictor):\n\n1ggplot2::ggplot(data = attend_tib, aes(x = year, y = n_lectures)) +\n2  geom_point(position = position_jitter(width = 0.1), alpha = 0.2, colour = \"yellowgreen\") +\n3  stat_summary(colour = \"steelblue\", fun.data = \"mean_cl_normal\") +\n4  scale_y_continuous(breaks = seq(0, 11, 1)) +\n  theme_light() \n\n\n1\n\nSet up the base layer. The predictor year goes on the x axis, the outcome goes on the y axis.\n\n2\n\nAdd the points. We’re tweaking the default settings so the distribution of the points is a little easier to see. For position we’re setting “position_jitter” which ensures the points are not stacked on top of each other. The width argument defines how far out the points spread. We’re also changing the transparency alpha to 0.2 so we can see more of the points. Finally, we change the colour.\n\n3\n\nstat_summary adds the means and the confidence intervals - we request this by specifying “mean_cl_normal” in the fun.data argument. When choosing colour, pick something that contrasts well with the scatter points in the background.\n\n4\n\nScale the y axis so it only shows whole numbers.\n\n\n\n\n\n\n\n\n\n\n\nBased on the plot, it seems like Year 1 students have the best attendance (though also have the fewest data points and therefore the widest confidence intervals), followed by Year 2 and Year 3, who attend the fewest sessions.\n\n\n\n\n\n\nFYI: Plotting a continuous predictor\n\n\n\n\n\nIn case we need to plot a continuous predictor, we can add a line of best fit in the same way as we did for the Gamma model, changing the methods.args option to a different distribution. For example:\n\nggplot2::ggplot(data = attend_tib, aes(x = overall_grade, y = n_lectures)) + \n  geom_point(position = position_jitter(width = 0.1), alpha = 0.2, colour = \"yellowgreen\") + \n  stat_smooth(method = \"glm\", colour = \"steelblue\", method.args = list(family = \"poisson\")) + \n  theme_light() \n\ncode `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelling counts</span>"
    ]
  },
  {
    "objectID": "01b_gzlms_counts.html#poisson-models",
    "href": "01b_gzlms_counts.html#poisson-models",
    "title": "2  Modelling counts",
    "section": "2.1 Poisson models",
    "text": "2.1 Poisson models\nMore often than not, a distribution is defined by some version of two parameters - location and dispersion. Location tells us at which point to centre the distribution and dispersion tells us how far the points spread out. In a Gaussian distribution, we have the mean and the standard deviation. In Gamma models, we have shape and scale. They’re not exactly the same, but they serve a similar function.\nPoisson models are simple creatures. They are only defined by one parameter \\(\\lambda\\) (lambda) which represents both the location and the dispersion. This is because of the mean-variance relationship the models assume:\n\\[\nV(Y) = E(Y)\n\\]\nIn which the variance of Y (the outcome) will always be equal to the expected value of Y. This is in contrast with the Gaussian models, which don’t assume a specific value as long as it’s constant (homoscedastic), or the Gamma models in which variance is equal to the square of the expected value.\nAs a result, they’re nice and simple to simulate:\n\n1poisson_dist &lt;- rpois(n = 1000, lambda = 1)\n\n2ggplot2::ggplot(data = NULL, aes(x = poisson_dist)) +\n3  geom_histogram()\n\n\n1\n\nGenerate 1000 random numbers drawn from a Poisson distribution with the lambda parameter equal to 1. Save the result into an object called poisson_dist .\n\n2\n\nAdd poisson_dist into the base layer of a plot\n\n3\n\nDraw a histogram.\n\n\n\n\n\n\n\n\n\n\n\nAs expected, the distribution only generates integers (or whole numbers). We can confirm this with the head() function, which will print out the first six values from the object we enter into it:\n\nhead(poisson_dist)\n\n[1] 0 0 0 1 2 1\n\n\nThe distribution is also right skewed. A Poisson distribution can only be right skewed or symmetrical. The larger the lambda parameter, the more symmetrical the distribution will be:\n\npoisson_dist &lt;- rpois(n = 1000, lambda = 10) \n\nggplot2::ggplot(data = NULL, aes(x = poisson_dist)) + \n  geom_histogram() \n\n\n\n\n\n\n\n\nNote that even if the distribution achieves perfect symmetry, we still can’t call it normal, because the values are not on a continuous scale (and there are other requirements a normal distribution has, other than symmetry).\nConversely, lambda can be smaller than 1, in which case the skew becomes more extreme and values will be bunched up around the lower end of the scale:\n\npoisson_dist &lt;- rpois(n = 1000, lambda = 0.5) \n\nggplot2::ggplot(data = NULL, aes(x = poisson_dist)) + \n  geom_histogram() \n\n\n\n\n\n\n\n\nThis mean-variance relationship is very restrictive. Realistically, there aren’t many variables that strictly follow a Poisson distribution, but let’s not doom our model before we’ve given it a chance (spoiler alert…).\n\nFit the model\nWe can fit the model using the glm function.\n\nattend_poiss &lt;- glm(\n1  n_lectures ~ year + n_record + overall_grade,\n2  data = attend_tib,\n3  family = poisson(link = \"log\")\n)\n\n\n1\n\nSpecify the formula\n\n2\n\nSpecify the dataset\n\n3\n\nSet the family to be poisson . The link function is log - same as for the Gamma models.\n\n\n\n\n\n\nEvaluate the model fit\n\ncheck_model(attend_poiss)\n\n\n\n\n\n\n\n\nFirst, let’s go through the plots we already know.\n\nPosterior predictive check - we no longer see the smooth curves - this is not a problem, because we’re not working with continuous data. The predictions themselves are not great. The model predicts fewer counts for values 0, 1, and 2than observed. The predictions are fine from 3 onward, but that left tail is quite inaccurate.\nHomogeneity of variance - looks a little funky because of the criss-crossy pattern. Again, this is not a problem and caused by the fact that we’re working with counts. The line isn’t entirely flat, but overall not too wobbly. I’d say the model passes the vibe diagnostic check on this criterion.\nInfluential observations - nothing flagged here.\nCollinearity - all VIF values are low and not a cause for concern\nUniformity of residuals - the points wobble around the line.\n\nBased on the posterior predictive check and the residuals, it seems like the model is not a great fit. This is confirmed by the plot that we skipped, which checks for over-dispersion.\n\n\nOver-dispersion\nLet’s take a closer look at this plot:\n\n\n\n\n\n\n\n\n\nThe straight blue line represents what kind of variance the model expects for different predicted values. Recall that the mean-variance relationship is:\n\\[\nV(Y) = E(Y)\n\\]\nIt’s a little difficult to see on the plot, but the line is plotted in a way where the value 1 on the x axis corresponds to the value of 1 on the y axis. Value 2 on x represents to 2 on y, and so on. We need the variance in our model (the green line with the ribbon) to follow this line. We can see that this is not the case. The green line wiggles above the straight line most of the time. This means that there is more dispersion in our model then the Poisson model allows - it is over-dispersed.\nAlthough this case is pretty cut and dry, we can use the over-dispersion test from easystats to get a second opinion:\n\ncheck_overdispersion(attend_poiss)\n\ncode # Overdispersion test\ncode \ncode        dispersion ratio =    1.927\ncode   Pearson's Chi-Squared = 1115.938\ncode                 p-value =  &lt; 0.001\n\n\ncode Overdispersion detected.\n\n\nFor a Poisson model, we’d expect the dispersion ratio to be 1. If it’s above 1, the model is over-dispersed, if it’s below 1, it’s under-dispersed. A p-value tells us whether this deviation from 1 is statistically significant.\n\n\n\n\n\n\nSignificance tests of assumptions\n\n\n\nGenerally, significance tests of assumptions are not the best idea because they because they encourage black and white thinking (over-dispersed vs under-dispersed) instead of nuanced evaluation. The dispersion ratio itself is a useful indicator of the amount of dispersion we have, however the p-values can be misleading. In larger samples, they will be more likely to sound an alarm even if the violation is minor. They might be helpful in an ambiguous situation where the plot isn’t clear, but even then a problem in one diagnostic plot will likely be echoed in at least one other plot.\n\n\nUnder over-dispersion, the parameter estimates are valid, however the standard errors (and therefore confidence intervals and p-values) are not. On the whole, this model is clearly not well specified. We have some alternatives. Again, ideally we want to think about whether a distribution is suitable for our data before we even collected the data, and certainly before we start mucking about with the data in the analysis stage. But let’s say that we’ve miscalculated. What are our options? Well, we still have:\n\nNegative binomial models\nQuasi-Poisson models",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelling counts</span>"
    ]
  },
  {
    "objectID": "01b_gzlms_counts.html#negative-binomial-models",
    "href": "01b_gzlms_counts.html#negative-binomial-models",
    "title": "2  Modelling counts",
    "section": "2.2 Negative binomial models",
    "text": "2.2 Negative binomial models\nNegative binomial (NB) distribution allows us to model counts in situations where dispersion is is greater than the expected mean. Unlike Poisson, NB distribution has a separate dispersion parameter that allows the model to be a little more flexible. The variance (V) is modelled as:\n\\[\nV(Y) = E(Y) + \\frac{E(Y)^2}{k}\n\\]\nWhich is the sum of the expected value of Y plus the square of the expected value divided by the dispersion parameter k . The k parameter therefore controls how far the values spread out. The smaller the k, the larger the dispersion and the extent of skewness are going to be (because dividing by a small number results in a larger number).\nIn the code below, the theta argument represents the k parameter:\n\nnb_dist &lt;- MASS::rnegbin(n = 1000, mu = 1, theta = 0.5) \n\nggplot2::ggplot(data = NULL, aes(x = nb_dist)) + \n  geom_histogram() \n\n\n\n\n\n\n\n\nConversely, we can set theta to be very large, in which case the NB distribution will start to resemble the Poisson distribution. This is because we divide by a large number, so the fraction \\(\\frac{E(Y)^2}{k}\\) will result in a very small value, and the variance will be very close to the value of the mean.\n\nnb_dist &lt;- MASS::rnegbin(n = 1000, mu = 1, theta = 100) \n\nggplot2::ggplot(data = NULL, aes(x = nb_dist)) + \n  geom_histogram() \n\n\n\n\n\n\n\n\n\nFit the model\nLet’s try this again, shall we? The function is a little different this time - we’re using the glm.nb from the package MASS to fit a negative binomial model. We still need to specify the formula and the data, but we don’t need to specify the family because glm.nb function can only fit negative binomial models:\n\nattend_nb &lt;- MASS::glm.nb(\n  n_lectures ~ n_record + year + overall_grade, \n  data = attend_tib\n)\n\n\n\nEvaluate the model fit\n\ncheck_model(attend_nb)\n\n\n\n\n\n\n\n\n\nPosterior predictive check - this is a much better fit then before.\nHomogeneity of variance: looks fine\nInfluential cases: none detected\nCollinearity: none detected\nResiduals - follow the line nicely.\nDispersion… wrong way around! This time, the variance is increasing at a slower rate than the model expects, as the green line with the ribbon departs from the predicted blue line around 1.5 on the x axis and continues below it. This is called under-dispersion. Again. the predictions and parameter estimates might be accurate, but if we care about inferential stats (let’s say that we do), we’re running the risk of basic our decisions on inaccurate p-values.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelling counts</span>"
    ]
  },
  {
    "objectID": "01b_gzlms_counts.html#quasi-poisson-models",
    "href": "01b_gzlms_counts.html#quasi-poisson-models",
    "title": "2  Modelling counts",
    "section": "2.3 Quasi-Poisson models",
    "text": "2.3 Quasi-Poisson models\nA third type of model we’re going to try is the Quasi-Poisson model. Strictly speaking, Quasi-Poisson models don’t belong to the GLM family, but they are very GLM like, in that:\n\nThey apply linearising link function to model the relationships of interest. This function is the natural logarithm, same as for Gamma and Poisson models.\nThey assume a mean-variance relationship, specifically that \\(V(Y) = \\phi E(Y)\\) . In Poisson models, the \\(\\phi\\) parameter is assumed to be 1, and therefore the variance is equal to the expected value. In Quasi-Poisson models, \\(\\phi\\) is estimated from the data and can be different from 1. The larger the \\(\\phi\\) the larger the variance.\nWe fit them with the glm function.\n\nUnfortunately, we can’t really simulate a Quasi-Poisson distribution to show how it changes with a changing \\(\\phi\\), because such a distribution doesn’t exist. In this sense, the models are different from GLM:\n\nBecause a Quasi-Poisson distribution doesn’t exist, the estimation cannot be based on Maximum Likelihood. It is based on Quasi-Likelihood.\nInstead, the parameters are estimated in the same way, but in addition we need to estimate the \\(\\phi\\) parameter, which is then used to adjust the standard errors, ensuring the inference (i.e. the p-values) is valid.\n\nBecause of the way \\(\\phi\\) is estimated, the model can deal with over- or under dispersion. If we were to use robust models as a metaphor for Quasi-likelihood models, they’re a similar level of fix as heteroscedasticity-consistent standard errors (e.g. “HC4”, “HC5”) - when we apply the HC4 correction, the parameter estimates are not affected, but the standard errors change because they are estimated from the model predictor matrix rather then based on an assumption of constant variance.\n\n\n\n\n\n\nThe drawbacks of Quasi-models\n\n\n\n\n\nThe general guidance is that we should use negative-binomial models when we have over-dispersed data, and only resolve to Quasi-Poisson if the issues with dispersion aren’t fixed under the NB distribution.\nMain reason being is that Quasi-likelihood models rob us of some of the benefits of full-likelihood models, such as the ability to compare models and evaluate improvement after adding predictors.\nFor example, imagine that we built our NB models bit by bit, by adding the predictors one after another:\n\nattend_0 &lt;- MASS::glm.nb(n_lectures ~ 1, data = attend_tib)\nattend_1 &lt;- MASS::glm.nb(n_lectures ~ year, data = attend_tib)\nattend_2 &lt;- MASS::glm.nb(n_lectures ~ year + n_record, data = attend_tib)\nattend_3 &lt;- MASS::glm.nb(n_lectures ~ year + n_record + overall_grade, data = attend_tib)\n\nWe can then compare these models based on AIC, BIC, and pseudo R2 :\n\ncompare_performance(attend_0, attend_1, attend_2, attend_3, metrics = \"common\") |&gt; \n  display()\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC (weights)\nBIC (weights)\nNagelkerke’s R2\nRMSE\n\n\n\n\nattend_0\nnegbin\n1861.1 (&lt;.001)\n1869.8 (6.00e-03)\n2.04e-15\n1.64\n\n\nattend_1\nnegbin\n1843.4 (0.01)\n1860.9 (0.48)\n0.06\n1.61\n\n\nattend_2\nnegbin\n1845.3 (5.00e-03)\n1867.2 (0.02)\n0.06\n1.61\n\n\nattend_3\nnegbin\n1834.6 (0.98)\n1860.8 (0.49)\n0.09\n1.60\n\n\n\n\n\nWe can see that Model 2 (in which we added the n_record predictor) is not really an improvement compared to the previous models. Generally, we’re looking for a model with the lowest AIC and BIC. We can also see that the \\(R^2\\) value doesn’t change from Model 1 to Model 2, but does improve for Model 3.\nWe can confirm this with a likelihood ratio test:\n\nanova(attend_0, attend_1, attend_2, attend_3) |&gt; \n  display(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\ntheta\nResid. df\n2 x log-lik.\nTest\ndf\nLR stat.\nPr(Chi)\n\n\n\n\n1\n1.212\n583\n-1857.067\n\n\n\n\n\n\nyear\n1.342\n581\n-1835.397\n1 vs 2\n2\n21.670\n1.970e-05\n\n\nyear + n_record\n1.342\n580\n-1835.317\n2 vs 3\n1\n0.081\n0.777\n\n\nyear + n_record + overall_grade\n1.422\n579\n-1822.621\n3 vs 4\n1\n12.695\n3.666e-04\n\n\n\n\n\nHere, the last column contains the p-value that tells us whether each model is a significant improvement compared to the previous model. The p-value for Model 2 is not statistically significant.\nIf likelihood-based comparisons are something we’re interested in, then we cannot use Quasi-Poisson model for estimation. But if we only want to estimate the parameters and make valid inference, they’re a good solution for dealing with over- or under-dispersion.\n\n\n\n\nFit the model\n\nattend_qpoiss &lt;- glm(\n  n_lectures ~ year + n_record  + overall_grade, \n  data = attend_tib, \n  family = quasipoisson(link = \"log\")\n)\n\n\n\nEvaluate the model fit\nWe don’t actually gain any additional insight from the model_check function - it will still tell us that there is over-dispersion based on the Poisson expectations, and the residuals will still indicate a suboptimal fit (because the check also expects a Poisson distribution). The posterior predictive check disappears, because there’s no theoretical distribution based on which to make predictions. The model fit statistics will also be based on the Poisson distribution, not on Quasi-Poisson. The only thing that is unique to Quasi-Poisson is the standard errors and p-values.\n\ncheck_model(attend_qpoiss)\n\n\n\n\n\n\n\n\nThis highlights the fact that a Quasi-Poisson is a “fix” to a problem and more of contingency, rather than a model that can be thoughtfully planned in advance. By default, we should aim for Poisson if it’s realistic, Negative-Binomial if we’re expecting over-dispersion, and only resort to Quasi-Poisson if the original plan fails.\nEither way, our options are not ideal. The NB model offers more accurate prediction but the standard errors and p-values will be inaccurate. Conversely, the Quasi-Poisson model generates valid inferential stats but the prediction will not be as accurate, especially for individuals with the lowest attendance.\nWe’re going stick with the Quasi-Poisson model for now, just to go over what the output looks like. The interpretation of the parameter estimates for all the models introduced so far is the same.\n\nperformance(attend_qpoiss) |&gt; \n  display()\n\n\n\n\nNagelkerke’s R2\nRMSE\nSigma\nScore_log\nScore_spherical\n\n\n\n\n0.13\n1.60\n1.39\n-1.67\n0.04\n\n\n\n\n\n\ntest_wald(attend_qpoiss) |&gt; \n  display(digits = 3)\n\n\n\n\nName\nModel\ndf\ndf_diff\nF\np\n\n\n\n\nNull model\nglm\n583\n\n\n\n\n\nFull model\nglm\n579\n4\n9.091\n&lt; .001\n\n\n\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\n\nOverall, the model explains 12.71% percent of deviance and is a significant improvement over the null model F(4,579) = 9.09, p &lt; .001.\n\n\nExtract model parameters\nOnce again, we exponentiate the parameter estimates so that we can interpret them with reference to the original scale:\n\nattend_qpoiss |&gt; \n  model_parameters(exponentiate = TRUE) |&gt; \n  display(digits = 3)\n\n\n\n\nParameter\nIRR\nSE\n95% CI\nt(579)\np\n\n\n\n\n(Intercept)\n0.989\n0.308\n(0.534, 1.809)\n-0.034\n0.973\n\n\nyear (Year 2)\n0.631\n0.094\n(0.476, 0.854)\n-3.087\n0.002\n\n\nyear (Year 3)\n0.346\n0.084\n(0.212, 0.550)\n-4.384\n&lt; .001\n\n\nn record\n1.008\n0.030\n(0.950, 1.070)\n0.267\n0.789\n\n\noverall grade\n1.013\n0.004\n(1.006, 1.020)\n3.452\n&lt; .001\n\n\n\n\n\n\nIf we wanted to, we could calculate the percentage change for all the parameters and their confidence intervals, by folowing the steps in the section Parameter estimates from the previous tutorial. Here, we’ll just focus on the estimates relevant to the hypothesis.\n\n\nInterpretation\nThe main predictor has three categories. We haven’t specified any contrasts2 so the output will compare two categories against the baseline. In our case, we previously set Year 1 to be the baseline, so Years 2 and 3 are compared against it.\nThe estimate for the Year 2 vs Year 1 comparison is 0.631. It is smaller that 1, which means that lecture attendance was lower for Year 2. We can convert it into a percentage by multiplying by 100, getting 63.1. This means that Year 2 attendance represents 63.1 percent of Year 1 attendance. If we wanted to calculate the difference, we subtract 100: 63.1 - 100 = -36.9. Therefore lecture attendance for Year 2 was -36.9% lower compared to Year 1. For Year 3, the attendance was -65.4% lower than Year 1 attendance.\nFor continuous predictors we’re back to a change in the outcome associated with a one unit increase in the predictor. For example, grade is positively associated with attendance. With each unit increase in grade, attendance increases by 1.3%. This seems small but remember that grade is measured on 100 point scale. We could generate some predictions to put this into perspective:\n\n1prediction_tib &lt;- tibble::tibble(\n2  year = \"Year 1\" |&gt; factor(levels = c(\"Year 1\", \"Year 2\", \"Year 3\")),\n3  n_record = 5.26,\n4  overall_grade = 30,\n)\n\npredict( \n  attend_qpoiss, \n  newdata = prediction_tib \n) |&gt; \n  exp()\n\n\n1\n\nCreate a new tibble which we’ll use for prediction.\n\n2\n\nFor any other variable type, we would just type the value for which we want to make prediction. However, the predict function requires variable types to be coded exactly as they are in the original dataset used for model fitting. So we need to first specify the value, then convert that value into a factor (because year is a factor) and specify the levels.\n\n3\n\nSet the number of recordings watched to the average, based on the table of descriptive stats we generated earlier.\n\n4\n\nSet the overall grade to 30 - for undergraduate modules, this is below the failing boundary of 40.\n\n\n\n\ncode        1 \ncode 1.518625\n\n\nThe model predicts that someone who is failing the module will attend 1.5 sessions, while other predictors are held constant. How about someone who’s acing the module?\n\nprediction_tib &lt;- tibble::tibble( \n  year = \"Year 1\" |&gt; factor(levels = c(\"Year 1\", \"Year 2\", \"Year 3\")), \n  n_record = 5.26, \n  overall_grade = 85, \n)\n\npredict( \n  attend_qpoiss, \n  newdata = prediction_tib \n) |&gt; \n  exp()\n\ncode      1 \ncode 3.0817\n\n\nA student who scored 85 on the module (more or less the maximum realistic grade in the UK higher education grading system) attends 3 lectures. Notwithstanding that attendance on this module seems to be extremely poor, that’s twice as many lectures compared to someone who’s under the failing boundary.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelling counts</span>"
    ]
  },
  {
    "objectID": "01b_gzlms_counts.html#zero-inflated-models-a-gentle-introduction",
    "href": "01b_gzlms_counts.html#zero-inflated-models-a-gentle-introduction",
    "title": "2  Modelling counts",
    "section": "2.4 Zero-inflated models (a gentle introduction)",
    "text": "2.4 Zero-inflated models (a gentle introduction)\nOur lecturer is in despair - there are so many students who don’t bother to attend even a single lecture. That’s terrible!\nBut then they remember an email sent to them by school office earlier that week:\n\nDear All,\nPlease remind the students to log their attendance on the app during your sessions. We’ve seen a considerable increase in students with zero lectures attended which is inconsistent with our alternative engagement monitoring records.\nThank you for your cooperation.\nThe School Office\n\nSo it is possible that students are attending the sessions, they’re just not logging them.\nThis is the sort of situation where zero-inflated models can help us understand some additional details about the relationships that we’re trying to model. Zero-inflation occurs when the amount of zeros observed is higher than the amount of zeros expected in a given distribution. Importantly, these zeros must be generated by a separate process (as opposed to just us mis-specifying the distribution).\nImagine you want to measure how much time people spend using a meditation app you’ve developed. You notice that many users have the app active for over an hour in a day, but the overall averages are low because of large spike at 0 - it seems many people don’t engage with the app on a daily basis. This might be due to various reasons - perhaps meditation is not part of their routine, perhaps they’re busy, perhaps they downloaded the app just after the New Year believing that this is the app that will get them into the habit of meditating. Either way, the outcome values are a result of two different processes:\n\nWhether or not the user engaged with the app in a given day\nIf they did, how long was their engagement.\n\nIn the lecture attendance example, it seems like zero-inflation is a plausible explanation - it might be that a lot of students are just not logging their attendance in the first place, so it makes no sense to model all these zeros together with the rest of our data.\nThis is where domain knowledge becomes important. We can test for zero-inflation, but the test doesn’t know the difference between a mis-specified model due to distribution and due to zero inflation. For example, we can use the check_zeroinflation() function on the Poisson Model:\n\ncheck_zeroinflation(attend_poiss)\n\ncode # Check for zero-inflation\ncode \ncode    Observed zeros: 239\ncode   Predicted zeros: 166\ncode             Ratio: 0.69\n\n\ncode Model is underfitting zeros (probable zero-inflation).\n\n\nThe result indicates the presence of zero-inflation. However, if we test on the negative-binomial model:\n\ncheck_zeroinflation(attend_nb)\n\n# Check for zero-inflation\n\n   Observed zeros: 239\n  Predicted zeros: 237\n            Ratio: 0.99\n\n\nModel seems ok, ratio of observed and predicted zeros is within the\n  tolerance range (p = 0.984).\n\n\nit seems like there’s no inflation, because the NB models is able to predict the zeros well. Therefore we need some sort of background knowledge that would allow to decide whether there is another process generating the zeros. Under the assumption that some students forget to (or just don’t want to) log their attendance when they are present in a session, we can fit a zero-inflated Poisson model. We’ll need to use the pscl package:\n\nattend_zi &lt;- pscl::zeroinfl(\n  n_lectures ~ year + n_record  + overall_grade |  overall_grade,\n  data = attend_tib, \n  dist = \"poisson\"\n)\n\nThe format of the function is very similar, however note that the formula is now split into two parts with a straight line | . The predictors that follow the straight line are the ones we believe can explain the zero inflation in the model3 (in this case, overall grade). The dist argument is used as an equivalent of the family argument in glm() .\nThe over-dispersion function from easystats doesn’t work with these zero-inflated models, but we can calculate dispersion ratio manually with a custom function4:\n\ndispersion_ratio &lt;- function(model){\n  E2 &lt;- resid(model, type = \"pearson\")\n  N  &lt;- nrow(model$model)\n  p  &lt;- length(coef(model)) \n  sum(E2^2) / (N - p)\n}\n\ndispersion_ratio(attend_zi)\n\n[1] 1.280356\n\n\nThe ratio is still above 1, suggesting we have some over-dispersion left in the model but it is much smaller than it was before.\n\nattend_zi |&gt; \n  performance() |&gt; \n  display()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\nScore_log\nScore_spherical\n\n\n\n\n1881.7\n1881.9\n1912.3\n0.08\n0.07\n1.60\n1.61\n-1.60\n0.04\n\n\n\n\n\n\nThe R2 is actually lower than the one for the original Poisson model, however these metrics are difficult to compare across models that are not nested. We’re now trying to model something more complicated than before, so we might just not be as successful as we were in modelling the a simpler relationship.\nFinally, let’s request the parameter estimates:\n\nattend_zi |&gt; \n  model_parameters(exponentiate = TRUE) |&gt; \n  display(digits = 3)\n\n\n# Fixed Effects\n\n\nParameter\nIRR\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n1.478\n0.384\n(0.888, 2.460)\n1.503\n0.133\n\n\nyear [Year 2]\n0.840\n0.094\n(0.674, 1.046)\n-1.560\n0.119\n\n\nyear [Year 3]\n0.451\n0.086\n(0.310, 0.655)\n-4.169\n&lt; .001\n\n\nn record\n1.000\n0.024\n(0.955, 1.048)\n0.011\n0.991\n\n\noverall grade\n1.008\n0.003\n(1.001, 1.015)\n2.238\n0.025\n\n\n\n\n# Zero-Inflation\n\n\nParameter\nOdds Ratio\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n1.236\n0.694\n(0.411, 3.715)\n0.378\n0.706\n\n\noverall grade\n0.979\n0.010\n(0.960, 0.999)\n-2.058\n0.040\n\n\n\n\n\n\nFirst thing to note, we have two sets of estimates - one for the Fixed Effects and one for Zero-Inflation. The fixed effects estimates are interpreted in the same way, although note that they’ve slightly changed. For example, the difference between Year 2 and Year 1 attendance is now only 16% (as opposed to 27% in the Quasi-Poisson model) and not statistically significant. This makes sense - we’re now modelling some of the non-attendance with zero-inflation.\nZero-inflation estimates tell us whether the predictors we specified can actually explain the excess of zeros. In effect, it is a logistic model with binary outcome, predicting whether the attendance was zero or another value, modelling the probability of a non-zero count.5 Logistic models use the logit link, which means that exponentiating the parameter estimates converts them into odds ratios. An odds ratio (OR) of 1 means zeros and non-zeros have equal odds of occurring regardless of the levels of the predictor. OR greater than 1 means that with each unit increase in the predictor, the odds of non-zeros increase. Conversely, OR smaller than zero means that the odds decrease.\nA brief reminder that odds are not the same as probability - it’s a ratio of probabilities:\n\\[\n\\mbox{odds} = \\frac{p}{1-p}\n\\]\nOdds are calculated as a ratio of the probability that an event occurs over the probability that it doesn’t. Consequently, odds ratio is the ratio of odds:\n\n\\[\nOR = \\frac{\\mbox{odds of an event at one level of the predictor}}{\\mbox{odds of an event at another level of the predictor}}\n\\]\nIn our case, overall grade had a negative relationship with attendance (OR &lt; 1). Similar to prevalence rate ratios (or “incidence rate ratios”), we can calculate the percentage change. But this time it’s not the percentage change in the value of the outcome but in the odds of a non-zero value. We take the exponentiated parameter estimate 0.979, and calculate (0.979 - 1) \\(\\times\\) 100 = -2.1. Therefore with each additional point in grade, the odds of attending the session (or at least logging attendance) decrease by 2.1 percent.\n\nDeciding between models\nWe’ve introduced a handful of models in this tutorial. Unfortunately, there is no hard-and-fast way to compare them with some statistical test. There are way to compare models based on likelihood ratio tests or overall fit statistics (for an example, see section The drawbacks of Quasi-models ), but we cannot use these for a comparison across models that model entirely different distributions. Deciding which model to use is a skill that takes practice, context knowledge, and experience. Here are some things to consider for count models and more generally.\n\nWhenever possible, the decision should be made before we even start data collection (or at least before we start analysing the data).\nWhat is the distribution of the variable likely going to look like? Is it bound on either side? Is it continuous or discrete? Is skewness likely?\nFor count models, is Poisson realistic? If you simulate distributions with different lambdas, do they ever achieve a realistic shape?\nIf equivalent mean-variance relationship cannot be assumed and you expect high dispersion, negative-binomial model is more appropriate.\nConsider Quasi-Poisson if the dispersion is too high for Poisson and too low for Negative-Binomial. This is more of a post-hoc solution rather than an ad-hoc plan.\nAlternatively, if an under-dispersed Negative-Binomial model is making accurate predictions and you don’t care about hypothesis testing, stick with it instead of a Quasi-model.\nWhen planning analysis, consider how the data are collected. Is there any mechanism that could generate an excess of zeros? If so, plan a model accounting for zero-inflation with an appropriate distribution family.\n\n\n\nReport\nCount models are very similar in interpretation to Gamma models - you can use the report example as a guideline. Here is a reminder on what to report and some tips:\n\nBrief summary of key insights from descriptive statistics and visualisation.\nState what sort of model you fitted.\nSummarise any assumption checks, highlight potential problems if there are any.\nReport and interpret model fit statistics, i.e. the F test and a pseudo R2\nReport parameter estimates\n\nReport and interpret the betas. For effects described in percentage change, try to contextualise the size of the effect (optionally with a prediction)\nReport confidence intervals - consider fully interpreting them, especially if either end of an interval challenges the conclusion that would be drawn from a beta alone.\nReport the standard error, the t statistic, and the p-value\nIf you only have a handful of predictors, report them all. If you have many of them, it’s fine to focus on the key predictors stated in your hypothesis, and provide a summary description for the effects of others (e.g. “all the covariates had a moderate positive association with the outcome, ps &lt; .001”), while pointing reader to a table where they find the full results.\nFor zero-inflated models report and interpret both the fixed effects and zero-inflation.\n\nSummarise the findings, referring back to the hypothesis.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelling counts</span>"
    ]
  },
  {
    "objectID": "01b_gzlms_counts.html#exercises",
    "href": "01b_gzlms_counts.html#exercises",
    "title": "2  Modelling counts",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\n\nWhat does this code do?\n\n\n\n\n\n\nHere’s all the code we have written in this section. Can you remember what each line of each codechunk does? Are there any codechunks that you struggle to make sense of? Make sure to revisit the section in which it is used and take notes.\n\n\n\n\nattend_tib &lt;- attend_tib |&gt; \n  dplyr::mutate(\n    year = factor(year)\n  )\n\n\nlevels(attend_tib$year)\n\n\ndescribe_distribution(attend_tib, include_factors = TRUE) |&gt; \n  display()\n\n\nggplot2::ggplot(data = attend_tib, aes(x = n_lectures)) + \n  geom_histogram(fill = \"yellowgreen\", alpha = 0.4) + \n  geom_line(stat = \"count\") + \n  geom_point(size = 2, stat = \"count\") + \n  scale_x_continuous(breaks = seq(0, 11, 1)) +  \n  theme_light() \n\n\nggplot2::ggplot(data = attend_tib, aes(x = year, y = n_lectures)) + \n  geom_point(position = position_jitter(width = 0.1), alpha = 0.2, colour = \"yellowgreen\") + \n  stat_summary(colour = \"steelblue\", fun.data = \"mean_cl_normal\") + \n  scale_y_continuous(breaks = seq(0, 11, 1)) +  \n  theme_light() \n\n\nggplot2::ggplot(data = attend_tib, aes(x = overall_grade, y = n_lectures)) + \n  geom_point(position = position_jitter(width = 0.1), alpha = 0.2, colour = \"yellowgreen\") + \n  stat_smooth(method = \"glm\", colour = \"steelblue\", method.args = list(family = \"poisson\")) + \n  theme_light() \n\n\npoisson_dist &lt;- rpois(n = 1000, lambda = 1) \n\nggplot2::ggplot(data = NULL, aes(x = poisson_dist)) + \n  geom_histogram() \n\n\nattend_poiss &lt;- glm(\n  n_lectures ~ year + n_record + overall_grade, \n  data = attend_tib, \n  family = poisson(link = \"log\") \n)\n\n\ncheck_model(attend_poiss)\n\n\ncheck_overdispersion(attend_poiss)\n\n\nnb_dist &lt;- MASS::rnegbin(n = 1000, mu = 1, theta = 0.5) \n\nggplot2::ggplot(data = NULL, aes(x = nb_dist)) + \n  geom_histogram() \n\n\nattend_nb &lt;- MASS::glm.nb(\n  n_lectures ~ n_record + year + overall_grade, \n  data = attend_tib\n)\n\n\nattend_qpoiss &lt;- glm(\n  n_lectures ~ year + n_record  + overall_grade, \n  data = attend_tib, \n  family = quasipoisson(link = \"log\")\n)\n\n\nperformance(attend_qpoiss) |&gt; \n  display()\n\n\ntest_wald(attend_qpoiss) |&gt; \n  display(digits = 3)\n\n\nattend_qpoiss |&gt; \n  model_parameters(exponentiate = TRUE) |&gt; \n  display(digits = 3)\n\n\nprediction_tib &lt;- tibble::tibble( \n  year = \"Year 1\" |&gt; factor(levels = c(\"Year 1\", \"Year 2\", \"Year 3\")), \n  n_record = 5.26, \n  overall_grade = 30, \n)\n\npredict( \n  attend_qpoiss, \n  newdata = prediction_tib \n) |&gt; \n  exp()\n\n\nprediction_tib &lt;- tibble::tibble( \n  year = \"Year 1\" |&gt; factor(levels = c(\"Year 1\", \"Year 2\", \"Year 3\")), \n  n_record = 5.26, \n  overall_grade = 85, \n)\n\npredict( \n  attend_qpoiss, \n  newdata = prediction_tib \n) |&gt; \n  exp()\n\n\ncheck_zeroinflation(attend_poiss)\n\n\nattend_zi &lt;- pscl::zeroinfl(\n  n_lectures ~ year + n_record  + overall_grade |  overall_grade,\n  data = attend_tib, \n  dist = \"poisson\"\n)\n\n\n\nWorksheet\n\nA social psychologist studies the predictors of popularity on social media. They randomly selects 212 BlueSky users and extracts information about their accounts and posting habits. Popularity is conceptualised by the number of followers (n_followers). The predictors to test are:\n\nFrequency of posting (frequency): Factor with levels “Less than once a day”, “Once a day”, “More than once a day”.\nPrimary post type (post_type): Factor with levels “Unique posts” and “Reposts”\nProportion of posts containing cute animal photos or videos (animal_posts) measured as a percentage.\n\nThe researcher hypothesises that:\n\nH1: Frequent posters will have more followers compared to those posting less frequently.\nH2: Users who primarily post unique posts rather than reposts will have more followers.\nH3: Users who post content related to cute animals more frequently will have more followers.\n\nThe researcher gets started on the analysis. So far they have fitted a Poisson model and a Gamma model. They think the Gamma model is a better fit, but they want you to look over the output and suggest improvements if needed. The output for each model is displayed below.\n\nThe data are stored in the file follower_data.csv .\n\n\n\n\n\n\nUse the tutorial to complete the following tasks:\n\nReview the output provided to you.\nIf there is an alternative model that can be fitted, do so.\nCompare all the outputs and select the best fitting model. Articulate your decision.\nExtract overall fit statistics and parameter estimates from the model you selected.\n(Optional) Create data visualisation to help you with interpretation.\nWrite up the results is a short report.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson model output:\n\n\n\n\n\n\n\n\n\n\n\nGamma model output:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional task\n\n\n\nWhat does the model you consider to be the best fitting predict for a BlueSky user who only posts unique posts with cute animals more than once per day?\n\n\n\n\n\nCheck worksheet values\nOnce you’ve finished the worksheet, you can ask me to look through your work and give you feedback. Remember that you should also practice writing up the results in a brief report, not just running the code. If you’re stuck, you can use the quiz below to guide you.\nYou can also use the quiz below - if you fitted the models the correctly, your answers should match the values below.\n\n\n\n\n\n\nWorksheet check\n\n\n\n\n\nBased on the diagnostic checks, the model that should be selected is:\n\n Gamma model Poisson model A different model\n\nTotal deviance explained: 70.3 %\nOverall F statistic: 2.76\nParameter estimates (IRR):\n\nPosting frequency (Once a day vs less than once a day): 1.71\nPosting frequency (More than once a day vs less than once a day): 2.53\nPost type (Unique posts vs reposts): 1.71\nPercentage of animal posts: 1.04\n\nThe results are in line with Hypothesis 1:\n\n TRUE FALSE\n\nThe results are in line with Hypothesis 2:\n\n TRUE FALSE\n\nThe results are in line with Hypothesis 3:\n\n TRUE FALSE\n\nOptional task:\nModel prediction: 1460.65",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelling counts</span>"
    ]
  },
  {
    "objectID": "01b_gzlms_counts.html#footnotes",
    "href": "01b_gzlms_counts.html#footnotes",
    "title": "2  Modelling counts",
    "section": "",
    "text": "In case someone messed up at the first hurdle and uploaded the wrong file…↩︎\nBut if we wanted to, we could do so in the same way as in an OLS model↩︎\nI’m not trying to make any statement here about how students with better or worse grades are more or less likely to log their attendance. Zero-inflated models are a pain to simulate so it just so happens that this variable ended up being a predictor. In reality, the variable that we specify as a predictor of zero inflation needs to have some theoretical justification.↩︎\nWe don’t really cover how to write custom functions on this module but if you’d like to know more, you can ask me in the session.↩︎\nIf you would find a recap of logistic regression helpful, you can revisit the DiscovR 19 tutorial (Categorical outcomes).↩︎",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelling counts</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html",
    "href": "01c_gzlms_ordinal.html",
    "title": "3  Modelling ordinal outcomes",
    "section": "",
    "text": "3.1 Types of Ordinal Models\nWe have three classes of ordinal models which are useful in different situations depending what kind of probability we’re trying to model and what sort of question we want to answer.\nIf it helps, you can think of them as different types of contrasts that allow us to make various comparison across levels. We often use simple dummy coding in these tutorials, but if we have a specific hypothesis about how levels might differ, we could apply custom contrasts.\nIn this tutorial, we’re going to focus on the Cumulative Models because (a) they are the most commonly used and (b) their interpretation is the closest to what we’d expect from the previous models we’ve covered.2 If you fancy a technical read, Bürkner and Vuorre (2019) explains the alternative models in more detail.\nAll in all, ordinal models are a bit of a doozy. Let’s jump straight into a scenario so we can introduce the necessary concepts with some practical context.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#types-of-ordinal-models",
    "href": "01c_gzlms_ordinal.html#types-of-ordinal-models",
    "title": "3  Modelling ordinal outcomes",
    "section": "",
    "text": "Cumulative models (or Proportional Odds Models) allow us to estimate the odds of the outcome being at a given level or lower against the odds of being at any remaining higher level.\nSequential models estimate the odds of the outcome being at the given level against the odds of being at any remaining higher level.\nAdjacent-category models estimate the odds of the outcome being at a given level against the odds of being at the next higher level.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#scenario",
    "href": "01c_gzlms_ordinal.html#scenario",
    "title": "3  Modelling ordinal outcomes",
    "section": "3.2 Scenario",
    "text": "3.2 Scenario\nThis scenario is loosely based on research we carried out a couple if years ago (Sladekova, Poupa, and Field 2024). The data are simulated.\n\nA team of meta-researchers wants know whether psychology researchers attend to the assumptions of the General Linear Model. They send out a survey asking researchers to complete a brief analysis exercise and upload their finished analytic scripts. The meta-researchers then go over the submitted surveys and code participants’ practice (assum_check) into the following categories\n\nNo assumption checks performed\nSuperficial or insufficient checks performed\nCorrect checks performed but no corrective follow-up action\nCorrect checks performed as well as a corrective follow-up action.\n\nAs predictors, they record the researchers’ seniority (seniority) and whether or not they teach statistics and research methods as part of their job (teach), hypothesising the following:\n\nH1: Senior researchers will attend to the GLM assumptions to a greater extent compared to junior researchers.\nH2: Researchers who teach research methods and statistics will attend to GLM assumptions to a greater extent compared to those who do not teach.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#cumulative-ordinal-models",
    "href": "01c_gzlms_ordinal.html#cumulative-ordinal-models",
    "title": "3  Modelling ordinal outcomes",
    "section": "3.3 Cumulative ordinal models",
    "text": "3.3 Cumulative ordinal models\nOne of the core principles of ordinal models is the assumption that there is underlying latent continuous variable. Each of our levels will cover a certain range within this variable, and the levels are differentiated by thresholds.\n\n\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\nIn our example, we’re assuming there is some latent variable that represents the attendance to assumptions. This is illustrated in Figure 3.1 . We have low attendance to assumptions on the left end, and high attendance on the right end. The vertical lines represent the thresholds between the levels (T1, T2, T3). We will always have one fewer thresholds than we have levels. In our case, we have four levels, therefore we need three thresholds in order to split the latent variable into four categories.\n\nThe thresholds\nThe way this latent variable is created under the hood is by applying a link function. The most commonly used link function with cumulative models is logit.\nIn a model with no predictors, we’re only estimating the position of the thresholds - if we were to move them left or right, the area under the curve for each level would change, and so would the probability of the outcome being from given level. The thresholds therefore represent the intercepts of the model. That’s right - cumulative models have multiple intercepts! Very exciting.\nSpecifically, we’re estimating:\n\nOdds of doing no checks VS odds of doing any checks (including a corrective action).\nOdds of doing no checks or insufficient checks VS odds of doing correct checks (including corrective action).\nOdds of doing no checks or any checks without action VS odds of doing correct checks with corrective action.\n\nWhen we add predictors, we’re estimating how these thresholds change in association with each predictor. A very neat thing is that even though we have all of these thresholds that need estimating, we’re only going to estimate one parameter for each predictor and assume that the way the odds outlined change is the same. We call this the proportional odds assumption - we’ll return to it in due course.\n\n\nPackages\nOnly familiar faces today - tidyverse, easystats and once again we’re summoning the MASS package which will help us fit ordinal models.\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(MASS)\n\n\n\nThe data\nThe data are stored in the file assumption_data.csv. Download the data and import it to Posit Cloud. Once you’ve done so, you can read the data into R by running:\n\nassum_tib &lt;- here::here(\"data/assumption_data.csv\") |&gt; readr::read_csv()\n\nWe’re going to be very deliberate with our levels this time - we need to tell R that our outcome is an ordered factor.\nWe also want to ensure the levels of our predictors are in the right order - our hypotheses make predictions about more senior researchers (H1) and researchers who teach stats (H2) so these are the levels we’ll set as the baseline.\n\nassum_tib &lt;- assum_tib |&gt; \n1  dplyr::mutate(\n2    assum_check = factor(\n      assum_check, \n3      levels = c(\"No checks\", \"Insufficient\", \"No follow-up\", \"Full checks\"),\n4      ordered = TRUE\n    ), \n5    seniority = factor(seniority, levels = c(\"Faculty\", \"PostDoc\", \"PhD\")),\n6    teach = factor(teach, levels = c(\"Yes\", \"No\"))\n  )\n\n\n1\n\nUse the mutate() function to modify variables.\n\n2\n\nUse the factor() function to convert assum_check into a factor.\n\n3\n\nList the levels in the right order.\n\n4\n\nSpecify that the levels of this factor have a meaningful order.\n\n5\n\nConvert seniority into a factor, with “Faculty” as the baseline.\n\n6\n\nConvert teach into a factor, with “Yes” as the baseline.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#descriptives",
    "href": "01c_gzlms_ordinal.html#descriptives",
    "title": "3  Modelling ordinal outcomes",
    "section": "3.4 Descriptives",
    "text": "3.4 Descriptives\nWe’re going to do something different here. All of our variables are categorical, so it makes little sense to calculate means, SDs and skewness (we could still run describe_distribution() if we had any continuous predictors though). Instead, we’re going to look at counts and relative percentages.\nWe’ll start with the teaching predictor. We want to know:\n(1) How many researchers completed different levels of assumption checking within each teaching category.\n(2) What percentage this represent within each teaching category.\n\nassum_desc_teach &lt;- assum_tib |&gt; \n1  dplyr::count(teach, assum_check) |&gt;\n2  dplyr::mutate(\n3    percent = n/sum(n)*100,\n4    .by = teach\n  )\n\nassum_desc_teach |&gt; \n  display()\n\n\n1\n\nUse the count() function to count cases within combinations of teaching levels and assumption checking levels.\n\n2\n\nUse the mutate() function to calculate a new variable\n\n3\n\nCalculate the percentage by dividing the count by the number of cases and multiplying by 100.\n\n4\n\nSplit the calculation above by the teach variable. This way the percentage calculation will be completed for teachers and non-teachers separately.\n\n\n\n\n\n\n\nteach\nassum_check\nn\npercent\n\n\n\n\nYes\nNo checks\n30\n17.75\n\n\nYes\nInsufficient\n34\n20.12\n\n\nYes\nNo follow-up\n49\n28.99\n\n\nYes\nFull checks\n56\n33.14\n\n\nNo\nNo checks\n59\n33.52\n\n\nNo\nInsufficient\n50\n28.41\n\n\nNo\nNo follow-up\n43\n24.43\n\n\nNo\nFull checks\n24\n13.64\n\n\n\n\n\n\nWe can do the same for the seniority variable:\n\nassum_desc_seniority &lt;- assum_tib |&gt; \n  dplyr::count(seniority, assum_check) |&gt;  \n  dplyr::mutate( \n    percent = n/sum(n)*100,\n    .by = seniority\n  )\n\nassum_desc_seniority |&gt; \n  display()\n\n\n\n\nseniority\nassum_check\nn\npercent\n\n\n\n\nFaculty\nNo checks\n24\n20.69\n\n\nFaculty\nInsufficient\n28\n24.14\n\n\nFaculty\nNo follow-up\n37\n31.90\n\n\nFaculty\nFull checks\n27\n23.28\n\n\nPostDoc\nNo checks\n33\n27.50\n\n\nPostDoc\nInsufficient\n31\n25.83\n\n\nPostDoc\nNo follow-up\n30\n25.00\n\n\nPostDoc\nFull checks\n26\n21.67\n\n\nPhD\nNo checks\n32\n29.36\n\n\nPhD\nInsufficient\n25\n22.94\n\n\nPhD\nNo follow-up\n25\n22.94\n\n\nPhD\nFull checks\n27\n24.77\n\n\n\n\n\n\nCalculating the percentage in this way allows us to make relative comparisons, even if the group sizes are not exactly equal. For example, we can see that 20.69% of the Faculty-level researchers completed no checks at all, however this was lower than the proportion of PostDocs in this category (27.50%) or the PhDs (29.36%)3 . Conversely, 23.28% if faculty ran full checks (including corrective action), compared to 21.67% of PostDocs and 24.77% of PhDs. Admittedly, it can be difficult to see any patterns in a table like this, so we might wish to visualise these percentages instead.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#visualise-the-data",
    "href": "01c_gzlms_ordinal.html#visualise-the-data",
    "title": "3  Modelling ordinal outcomes",
    "section": "3.5 Visualise the data",
    "text": "3.5 Visualise the data\nNotice that when created the summary tables above, we also saved them into a separate objects, assum_desc_teach and assum_desc_seniority . These will come in handy here.\nFor teaching:\n\n1ggplot2::ggplot(data = assum_desc_teach,\n2                aes(x = assum_check, y = percent,\n3                    colour = teach, linetype = teach, group = teach)) +\n4  geom_point(size = 2) +\n5  geom_line() +\n6  scale_colour_viridis_d(end = 0.9) +\n7  coord_cartesian(ylim = c(0, 100)) +\n  theme_light()\n\n\n1\n\nUsed the summary that we created in the previous step.\n\n2\n\nassum_check goes on the x axis, percent goes on the y axis.\n\n3\n\nWe’re going to create separate lines for each level of teach - we want them to be differentiated by colour and linetype. We’re also adding the group argument, which helps ggplot connect the lines correctly.\n\n4\n\nAdd points. Optionally change their sizes.\n\n5\n\nAdd the lines.\n\n6\n\nChange the colours to something more accessible. Here we’re using the viridis colour scheme, which is a delight to look at. Viridis goes from light yellow to dark purple. Yellow can be difficult to see on a light background - we’re setting the end argument to 0.9 (1 would go all the way to yellow), so the colour on the lower end stays slighly darker green.\n\n7\n\nWe’re working with percentages, so the y axis should go from 0 to 100.\n\n\n\n\n\n\n\n\n\n\nFigure 3.2\n\n\n\n\n\nFor seniority:\n\nggplot2::ggplot(data = assum_desc_seniority, \n                aes(x = assum_check, y = percent, \n                    colour = seniority, linetype = seniority, group = seniority)) + \n  geom_point(size = 2) + \n  geom_line() + \n  scale_colour_viridis_d(end = 0.9) + \n  coord_cartesian(ylim = c(0, 90)) +\n  theme_light()",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#fit-the-model",
    "href": "01c_gzlms_ordinal.html#fit-the-model",
    "title": "3  Modelling ordinal outcomes",
    "section": "3.6 Fit the model",
    "text": "3.6 Fit the model\nWe’ll use the polr function (which stands for Proportional Odds Linear Regression) from the MASS package to fit our model. All we need is the formula and the data:\n\nassum_ord &lt;- MASS::polr(assum_check ~ seniority + teach, data = assum_tib)",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#check-model-fit",
    "href": "01c_gzlms_ordinal.html#check-model-fit",
    "title": "3  Modelling ordinal outcomes",
    "section": "3.7 Check model fit",
    "text": "3.7 Check model fit\nLet’s start with the easystats approach:\n\ncheck_model(assum_ord)\n\n\n\n\n\n\n\n\nThe posterior predictive check looks pretty good - there’s a lot of uncertainty in the prediction but on average it aligns with our data. No problems with collinearity.\n\nProportional Odds Assumption\nRecall that our model estimates the following:\n\nOdds of doing no checks VS odds of doing any checks (including a corrective action).\nOdds of doing no checks or insufficient checks VS odds of doing correct checks (including corrective action).\nOdds of doing no checks or any checks without action VS odds of doing correct checks with corrective action.\n\nIt will also estimate how these odds change with our predictors. The model assumes that regardless of which threshold (T1, T2 or T3) we’re referring to, the effect of the predictor will be the same.\nApparently, there are formal tests and no doubt functions that can give us a simple yes/no answer as to whether the assumption is upheld. But formal tests are the huckster’s crutch and we’re no hucksters. We’re going to take the slow road, which will help us understand the assumption better.\nNotice that each item of the list above compares two odds. We can think of them three separate logistic models where the outcome has two categories. So here’s what we’re going to do:\n\nRecode the outcome into three variables, where each has only two levels.\nFit a logistic model to each of these outcomes.\n\n\nRecoding the outcome\nThe first logistic model compares the odds of doing no checks at all (level 1) against all other levels. We can recode outcomes using the case_when() function from dplyr. We’ll start with one variable so you get a sense of how it works, and then we’ll show the full code:\n\nassum_tib &lt;- assum_tib |&gt; \n1  dplyr::mutate(\n2    level_1_vs_levels_234 = dplyr::case_when(\n3      assum_check %in% c(\"No checks\") ~ 0,\n4      assum_check %in% c(\"Insufficient\", \"No follow-up\", \"Full checks\") ~ 1\n    )\n  )\n\n\n1\n\nUse the mutate() function to create a new variable.\n\n2\n\nWe’re calling this new variable level_1_vs_levels_234 to indicate which levels we’re comparing (you can call it anything as long as it makes sense to you). The way to create this new variable is to use the function case_when().\n\n3\n\nFor cases when assum_check has the value “No checks”, assign (~)the value 0.\n\n4\n\nFor cases when assum_check has the value “Insufficient” or “No follow-up” or “Full checks”, assign the value 1.\n\n\n\n\nIf you inspect the tibble, you’ll see that there’s now a new column at the end titled level_1_vs_levels_234 . We could have assign any two values here, but 0 and 1 are simple enough - we just need to remember that 0 represents worse checks and 1 represents better checks.\nWe can do all the recoding in just one chunk:\n\nassum_tib &lt;- assum_tib |&gt; \n  dplyr::mutate( \n    level_1_vs_levels_234 = dplyr::case_when( \n      assum_check %in% c(\"No checks\") ~ 0,  \n      assum_check %in% c(\"Insufficient\", \"No follow-up\", \"Full checks\") ~ 1\n    ), \n    levels_12_vs_levels_34 = dplyr::case_when( \n      assum_check %in% c(\"No checks\", \"Insufficient\") ~ 0,  \n      assum_check %in% c(\"No follow-up\", \"Full checks\") ~ 1\n    ), \n    levels_123_vs_level_4 = dplyr::case_when( \n      assum_check %in% c(\"No checks\", \"Insufficient\", \"No follow-up\") ~ 0,  \n      assum_check %in% c(\"Full checks\") ~ 1\n    )\n  )\n\n\n\nFitting the logistic models\nNow we’re ready to fit our three models. A reminder that when we fit logistic models, we do so with the glm() function. The family we need for binary outcomes is “binomial”. We keep the same formula as before, but we use the new recoded variables as the outcomes.\n\nmodel_log_1 &lt;- glm(level_1_vs_levels_234 ~ seniority + teach, data = assum_tib)\nmodel_log_2 &lt;- glm(levels_12_vs_levels_34 ~ seniority + teach, data = assum_tib)\nmodel_log_3 &lt;- glm(levels_123_vs_level_4 ~ seniority + teach, data = assum_tib)\n\nNice and easy. Next we summon the compare_models() function from easystats. This function will extract the parameter estimates and confidence from all three models, exponentiate them, and present them next to each other so we compare them. Nice and neat.\n\ncompare_models(model_log_1, model_log_2, model_log_3, exponentiate = TRUE) |&gt; \n  display(digits = 3)\n\n\n\n\n\n\n\n\n\n\nParameter\nmodel_log_1\nmodel_log_2\nmodel_log_3\n\n\n\n\n(Intercept)\n2.390 (2.182, 2.618)\n1.957 (1.766, 2.169)\n1.392 (1.275, 1.518)\n\n\nseniority (PostDoc)\n0.935 (0.838, 1.045)\n0.920 (0.812, 1.042)\n0.986 (0.887, 1.096)\n\n\nseniority (PhD)\n0.920 (0.822, 1.031)\n0.933 (0.821, 1.060)\n1.020 (0.915, 1.136)\n\n\nteach (No)\n0.855 (0.780, 0.937)\n0.787 (0.710, 0.872)\n0.823 (0.754, 0.898)\n\n\n\n\n\n\n\n\nObservations\n345\n345\n345\n\n\n\n\n\nFirst thing to note - the intercepts are very different from each other. That’s okay and expected - remember, the intercepts represent the thresholds, which naturally have different positions (Figure 3.1). The values that we’re interested in are the parameter estimates for the predictors, which tell us how the thresholds change. Let’s go row by row:\n\nseniority (PostDoc) - all estimates are in the 0.9 range. All confidence intervals also overlap with 1, meaning that that there might be no difference between PostDocs and Faculty (our baseline).\nseniority (PhD) - similar as above, all estimates 0.9 and confidence intervals overlap.\nteach (No) - For this effect, all estimates are around 0.8, indicating that the odds of “better practice” are lower for individuals who don’t teach stats. Again, confidence intervals overlap with each other, and none of them cross 1 so the estimation is consistent.\n\n\n\n\n\n\n\nVisualising the proportional odds assumption (optional)\n\n\n\n\n\nThis is an optional section showing how we can visualise the parameter estimates to compare them model easily. First, we’re going to scale back to the model_parameters() function and extract each set of estimates separately. We’re store these into a table, and then we’ll use that table with ggplot.\n\nparams_1 &lt;- model_parameters(model_log_1, exponentiate = TRUE) |&gt; \n1  dplyr::mutate(model = \"Model 1\")\nparams_2 &lt;- model_parameters(model_log_2, exponentiate = TRUE) |&gt; \n2  dplyr::mutate(model = \"Model 2\")\nparams_3 &lt;- model_parameters(model_log_3, exponentiate = TRUE) |&gt; \n3  dplyr::mutate(model = \"Model 3\")\n\n4prop_odds_tbl &lt;- rbind(params_1, params_2, params_3) |&gt;\n5  as.data.frame()\n\n\n1\n\nExtract the first set of estimates. Modify this table to have a new column called “model”. This column will indicate which model our estimates come from.\n\n2\n\nExtract estimates for the second model and add the model column.\n\n3\n\nExtract estimates for the third model and add the model column.\n\n4\n\nUse the rbind() function to bind the estimates from all three models into a single table. Store the result into an object called prop_odds_tbl .\n\n5\n\nConvert into a data frame so it’s easier to work with in ggplot.\n\n\n\n\nNow let’s create our plot.\n\n1prop_odds_tbl |&gt;\n  ggplot2::ggplot(data = _, aes(x = model, y = Coefficient, \n2                                ymin = CI_low, ymax = CI_high)) +\n3  geom_point() +\n4  geom_errorbar(width = 0.1) +\n5  facet_wrap(~Parameter, scales = \"free_y\") +\n  theme_light()\n\n\n1\n\nUse the table we’ve just created as the dataset.\n\n2\n\nThe model variable should be in the x axis so that we have estimates from different models next to each other. The y axis should have the Coefficient value, and we’re also specifying ymin and ymax so the error bars represent the confidence intervals.\n\n3\n\nAdd points.\n\n4\n\nAdd the error bar. Optionally, set the width to be 0.1 so the plot looks nicer.\n\n5\n\nAdd facet wrap so that we get a separate plot for each parameter. The scales = \"free_y\" ensures that the y axis adapts depending on the effect, instead of being fixed at the same values for all effects.\n\n\n\n\n\n\n\n\n\n\n\nLovely! This makes it a little easier to judge how similar or different the estimates are. The plots underpin our conclusion from above - even though the point estimates are a little different at times, the intervals overlap substantially.\n\n\n\nBased on this, we can conclude that the effects are equivalent across the thresholds, and therefore the proportional odds assumption has been satisfied. What sort of different would we have to find in order to violate the assumption? There’s no specified cut-off value and it’s an answer that requires our knowledge as a researcher, not as a statistician. Considering the kind of outcome that we’re measuring, what sort of difference would meaningfully change our interpretation and conclusion? It’s not a straightforward answer, especially given that, as a field, we’re generally very obsessed with p-values and binary answers. If you find odds ratios unintuitive to think about, you can always generate some predictions and see whether they change across the models. You can give this a go if you wish, but I feel like we’ve spent enough time on a single assumption. Moving on!\n\n\n\nFit statistics\nLet’s get back to our model, shall we? We extract the fit statistics as usual:\n\nmodel_performance(assum_ord) |&gt; \n  display(digits = 3)\n\nCan't calculate log-loss.\n\n\n\n\n\nAIC\nAICc\nBIC\nNagelkerke’s R2\nRMSE\nSigma\n\n\n\n\n940.9\n941.1\n964.0\n0.079\n2.485\n1.648\n\n\n\n\n\n\ntest_wald(assum_ord)|&gt; \n  display(digits = 3)\n\n\nLikelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\n\nName\nModel\ndf\ndf_diff\nChi2\np\n\n\n\n\nNull model\npolr\n3\n\n\n\n\n\nFull model\npolr\n6\n3\n26.66\n&lt; .001\n\n\n\n\n\n\nThe model explains 7.9% of total deviance. If you run the Wald test in your own R session, you will get a message informing you that this test is inappropriate for models with categorical predictors, so the function provides the Likelihood Ratio Test using the \\(\\chi^2\\) statistic instead. This is okay, we just need to remember to report it as such (instead of reporting the F statistic).",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#extract-model-parameters",
    "href": "01c_gzlms_ordinal.html#extract-model-parameters",
    "title": "3  Modelling ordinal outcomes",
    "section": "3.8 Extract model parameters",
    "text": "3.8 Extract model parameters\nTime to get our parameters and try tointerpret them:\n\nmodel_parameters(assum_ord, exponentiate = TRUE) |&gt; \n1  knitr::kable(digits = 3)\n\n\n1\n\nThe display() function seems to struggle to organise the output sensibly, so we’ll just use this kable() (which is less intuitively named but works a treat).\n\n\n\n\ncode \ncode Re-fitting to get Hessian\n\n\n\n\nTable 3.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\nComponent\n\n\n\n\nNo checks|Insufficient\n0.167\n0.037\n0.95\n0.108\n0.258\n-8.055\n339\n0.000\nalpha\n\n\nInsufficient|No follow-up\n0.512\n0.104\n0.95\n0.344\n0.763\n-3.305\n339\n0.001\nalpha\n\n\nNo follow-up|Full checks\n1.813\n0.368\n0.95\n1.216\n2.703\n2.931\n339\n0.004\nalpha\n\n\nseniorityPostDoc\n0.777\n0.181\n0.95\n0.492\n1.225\n-1.085\n339\n0.279\nbeta\n\n\nseniorityPhD\n0.789\n0.192\n0.95\n0.490\n1.270\n-0.974\n339\n0.331\nbeta\n\n\nteachNo\n0.373\n0.074\n0.95\n0.252\n0.551\n-4.944\n339\n0.000\nbeta\n\n\n\n\n\n\n\n\n\nThresholds (intercepts)\nThe first three rows refer to the threshold estimates. They’re labelled as “alpha” in the Component column at the end. The coefficients represent the values of the thresholds when all the predictors are the the baseline, which would be the “Faculty” level for seniority and “Yes” for teaching. For these individuals:\n\nNo checks|Insufficient: The odds of doing no checks are lower by a factor of 0.167 compared to the odds of doing any checks at all.\nInsufficient|No follow-up: The odds of doing no checks or insufficient checks are lower by a factor of 0.512 compared to the odds of doing correct checks or correct checks with a follow-up action.\nNo follow-up|Full checks: The odds of doing no checks, insufficient checks, or correct checks are higher by a factor of 1.813 compared to the odds of doing correct checks with a follow-up action.\n\nAll of this makes sense - the last comparison might seen a little un-intuitive at first because (at least based on our hypothesis) the baseline categories are meant to be “the best of the best”, but think back to Figure 3.1 - We’re comparing the mass before T3 against the mass after it, so the odds are almost always going to be higher for this threshold.\nWe will unpick these in a painstaking detail in just a moment, but for now, let’s move on to the parameter estimates.\n\n\nParameter estimates\nThe parameter estimates are tagged with the “beta” label under the Component column. These estimates tell us how the thresholds change. Because of the proportional odds assumption, we’re assuming an equivalent change for each threshold. As a reminder, values below 1 indicate a negative relationship between variables, values above indicate a positive relationships, and 1 indicates no relationship.\n\nseniorityPostDoc: Compared to the Faculty, the PostDocs had lower odds of having their assumption checks categorised in the higher levels, by a factor of 0.777. This difference was not statistically significant. If you find positive odds ratios easier to think about, we can reverse the calculation as 1/0.777 = 1.287. This value can be interpreted in two ways (1) the Faculty odds of faculty performing checks in the higher levels were 1.287 times higher compared to the odds of PostDocs or (2) The PostDocs were 1.287 more likely to perform checks in the lower levels (or perform no checks at all). Both are mathematically valid.\nseniorityPhD: Compared to the Faculty, PhD researchers had lower odds of performing higher level checks by a factor of 0.789. Alternatively, the Faculty were 1/0.789 = 1.267 times more likely to performed higher level checks compared to PhDs. This difference was not statistically significant. The confidence intervals for this effect as well as the previous one contain the value 1, suggesting that no difference in assumption checking is plausible.\nteachNo: Compared to statistics instructors, the odds of performing higher level checks were lower for those who don’t teach by a factor of 0.373. Conversely, the odds of performing higher level checks were 2.681 times higher (1/0.373) for those who do teach statistics. This effect was statistically significant. The confidence intervals indicated that this increase in odds could be as large as 3.968 (1/0.252) or as low as 1.815 (1/0.551), assuming our confidence is one of the 95% containing the true population parameter.\n\nAs interpretations go, this is understandable enough. However, the origin of these odds rations is still a little mysterious so we’re going to dig a little deeper. What we’re often interested in with these models is the probability of performing certain level of checks at different levels of the predictors, but it’s difficult to intuit this just from the output above.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#unpicking-the-odds",
    "href": "01c_gzlms_ordinal.html#unpicking-the-odds",
    "title": "3  Modelling ordinal outcomes",
    "section": "3.9 Unpicking the odds",
    "text": "3.9 Unpicking the odds\nWe can use the emmeans package to request the predicted probabilities for each category. The function we’ll use streamlines the predict() function and generates predictions across different levels of a predictor, while holding other predictors constant.\n\n1assum_emm &lt;- emmeans::emmeans(\n2  assum_ord,\n3  specs = \"assum_check\",\n4  by = \"teach\",\n5  at = list(seniority = \"Faculty\"),\n6  mode = \"prob\"\n)\n\n\n1\n\nUse the emmeans() function from the emmeans package. We’ll be storing the result of this function into an object called assum_emm.\n\n2\n\nSpecify the model.\n\n3\n\nWe want to make predictions for different levels of assumption checks, so we specify assum_check in the specs argument.\n\n4\n\nSplit the prediction for different teaching and non-teaching participants.\n\n5\n\nSpecify where to hold the remaining predictors constant. We can add multiple variables here if we need to, so we need to wrap them in the list() function. We’ll hold seniority at the “Faculty” level because that’s our baseline.\n\n6\n\nThe mode argument ensures that the resulting predictions are converted into probabilities.\n\n\n\n\ncode \ncode Re-fitting to get Hessian\n\nassum_emm |&gt; \n  knitr::kable(digits=3)\n\n\n\nTable 3.2\n\n\n\n\n\n\nassum_check\nteach\nprob\nSE\ndf\nasymp.LCL\nasymp.UCL\n\n\n\n\n1\nYes\n0.143\n0.027\nInf\n0.089\n0.196\n\n\n2\nYes\n0.196\n0.026\nInf\n0.145\n0.246\n\n\n3\nYes\n0.306\n0.027\nInf\n0.253\n0.359\n\n\n4\nYes\n0.355\n0.047\nInf\n0.264\n0.447\n\n\n1\nNo\n0.309\n0.043\nInf\n0.223\n0.394\n\n\n2\nNo\n0.270\n0.026\nInf\n0.219\n0.321\n\n\n3\nNo\n0.251\n0.028\nInf\n0.196\n0.306\n\n\n4\nNo\n0.171\n0.031\nInf\n0.111\n0.231\n\n\n\n\n\n\n\n\nFirst, let’s just visualise these so we can spot the patterns better:\n\nassum_emm |&gt; \n1  as.data.frame() |&gt;\n  ggplot2::ggplot(data = _, aes(x = assum_check, \n2                                y = prob, ymin = asymp.LCL, ymax = asymp.UCL,\n3                                linetype = teach, group = teach)) +\n4  geom_point(size = 2) +\n5  geom_errorbar(width = 0.1) +\n6  geom_line() +\n7  coord_cartesian(ylim = c(0, 1)) +\n  theme_light()\n\n\n1\n\nConvert the emm object into a dataframe otherwise ggplot will request to speak to our manager.\n\n2\n\nPlace prob on the y axis, and the confidence intervals (asymp.LCL and asymp.UCL) on ymin and ymax. This will be useful for errorbars.\n\n3\n\nDifferentiate the teach variable by linetype.\n\n4\n\nAdd the points.\n\n5\n\nAdd the error bars. Optionally, change width to make the plot nicer.\n\n6\n\nAdd the lines.\n\n7\n\nChange the co-ordinates to be between 0 and 1 (because we’re working with probabilities, not percentages).\n\n\n\n\n\n\n\n\n\n\nFigure 3.3\n\n\n\n\n\nThat’s a lovely and clear plot. You might be thinking that it looks very similar to Figure 3.2, and you’d be right. The main difference is that Figure 3.2 shows percentages based on raw counts, while this figure is based on the model prediction while also accounting for the other predictors in the model AND it gives us nice error bars. In a report, you wouldn’t include both. I’d probably only include the descriptive tables (because they contain the same info as Figure 3.2), and then include the model-predicted probabilities to help with the interpretation of the results.\nNow that I’ve lulled you into a false sense of security with a plot, let’s do some maths. We want to understand how our probabilities in the figure above and in Table 3.2 relate to the parameter estimates in Table 3.1 .\nA reminder that odds are calculated as the ratio of a probability of some value Y occurring vs the inverse of that probability. For example, we could calculate the odds the first level of outcome (“No checks”) as:\n\\[\nOdds(Y=1) = \\frac{P(Y=1)}{1 - P(Y=1)}\n\\]\nOdds Ratio is ratio of odds. We calculate the odds of different levels and then calculate their ratio. Let’s say that Y = 2|3|4 represents a situation where the outcome is either the second, third, or fourth level. The odds are calculated as:\n\\[\nOdds(Y=2|3|4) = \\frac{P(Y=2|3|4)}{1 - P(Y=2|3|4)}\n\\]\nThen the odds ratio of the the first level against the other three levels is:\n\\[\nOR_{(\\mbox{1 vs 2|3|4})} = \\frac{Odds(Y=1)}{Odds(Y=2|3|4)}\n\\]\n\nCalculating the thresholds\nLet’s work with the values from Table 3.2 . For now, focusing on the “Yes” category for teaching:\n\nProbability of No checks (assum_check is 1): 0.143\nProbability of any checks at all (assum_check is 2 or 3 or 4): 0.196 + 0.306 + 0.355 = 0.857\nOdds (1 vs 2|3|4) = 0.143 / 0.857 = 0.167. This is the same value as the threshold No checks|Insufficient from Table 3.1\n\nLet’s try to work out the the second threshold:\n\nProbability that assum_check is 1 or 2: 0.143 + 0.196 = 0.339\nProbability that assum_check is 3 or 4: 0.306 + 0.355 = 0.661\nOdds (1|2 vs 3|4) = 0.339 / 0.661 = 0.152. This is the same value as the threshold Insufficient|No follow-up from Table 3.1\n\nFinally, the last threshold:\n\nProbability that assum_check is 1 or 2 or 3: 0.143 + 0.196 + 0.306 = 0.645\nProbability that assum_check is 4: 0.355\nOdds (1|2 vs 3|4) = 0.645 / 0.355 = 1.816. This is the same value as the threshold No follow-up|Full checks from Table 3.1 (allowing for minor rounding variation)\n\n\n\nCalculating the parameter estimates\nWe’re just going to stick with the teach variable here to keep things simple.\n\nThreshold 1 - No checks|Insufficient:\n\nOdds (1 vs 2|3|4) for those who teach = 0.167(we worked this out above)\nOdds (1 vs 2|3|4) for those who do not teach = 0.309 / (0.270 + 0.251 + 0.171) = 0.446 (we’re taking these probabilities from Table 3.2)\nOdds Ratio for teachNo: 0.167 / 0.446 = 0.374 which is the same as the Coefficient from Table 3.1 .\n\n\n\n\nThreshold 2 - Insufficient|No follow-up:\n\nOdds (1|2 vs 3|4) for those who teach = 0.512\nOdds (1|2 vs 3|4) for those who do not teach = (0.309 + 0.270) / (0.251 + 0.171) = 1.372\nOdds Ratio for teachNo: 0.512 / 1.372 = 0.374 which, again, is the same as the Coefficient from Table 3.1 .\n\nThreshold 3 - No follow-up|Full checks:\n\nOdds (1|2|3 vs 4) for those who teach = 1.813\nOdds (1|2|4 vs 4) for those who do not teach = (0.309 + 0.270 + 0.251) / 0.171 = 4.853\nOdds Ratio for teachNo: 1.813 / 4.853 = 0.374 same as above and Table 3.1 . These are proportional odds demonstrated in action. No matter which threshold we consider, the change in the odds for this specific predictor remains the same.\n\n\nIf you’ve read this far, well done. this Getting to grips with these models is not the easiest task but it’s worth having some understanding of where the values are coming from. Because if we know where values come from, we can (1) interpret them properly and (2) be able to judge when something goes wrong and the model presents us with non-sense results (3) be able to judge when someone is presenting us with non-sense results.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#report",
    "href": "01c_gzlms_ordinal.html#report",
    "title": "3  Modelling ordinal outcomes",
    "section": "3.10 Report",
    "text": "3.10 Report\nThe good news is that we don’t really need to go into an extensive detail as we did in the previous section when reporting and interpreting the results. Generally, a good middle ground is to report and interpret the odds ratios with reference to the hypotheses and then comment on estimated (predicted) probabilities generated by emmeans. Here’s an example of how we could approach this:\n\nThreshold estimates and parameter estimates are summarised in Table 3.1 . Compared to researchers at the Faculty level, both PostDocs (OR = 0.777, 95% CI [0.492, 1.225]) and PhD researchers (OR = 0.789, 95% CI [0.490, 1.270]) had lower odds of performing higher level assumption checks. Contrary to Hypothesis 1, these differences were not statistically significant at \\(\\alpha\\) = .05).\nResearchers who didn’t teach research methods and statistics had lower odds of performing higher level assumption checks (OR = 0.373, 95% CI [0.252, 0.551]). This reduction in odds was statistically significant, p &lt; .001. Conversely, this means the odds of performing higher level checks were 1/0.373 = 2.681 times higher for researchers who also teach statistics. This aligns with Hypothesis 2.\nFigure 3.3 shows the model predicted probabilities that confirm this trend - while the probabilities for teaching and non-teaching researchers overlapped for the middle two levels, there was a clear distinction in the lowest and the highest assumption-checking level. Researchers who teach were less likely to forgo assumptions p(“No checks”) = 0.143[0.089, 0.196], compared to those who don’t teach, p(“No checks”) = 0.309[0.223, 0.394]. Opposite trend was noted for the full assumption checks with a corrective action., which were more likely to be completed by researchers who teach statistics, p(“Full checks”) = 0.355[0.264, 0.447], than by non-teaching researchers, p(“Full checks”) = 0.171[0.111, 0.231].\n\nNote: you would of course also briefly comment on the descriptives, model checks, and fit statistics - here we’re just focusing on the novel part of parameter estimates for ordinal models.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#exercises",
    "href": "01c_gzlms_ordinal.html#exercises",
    "title": "3  Modelling ordinal outcomes",
    "section": "3.11 Exercises",
    "text": "3.11 Exercises\n\nWhat does this code do?\n\n\n\n\n\n\nHere’s all the code we have written in this section. Can you remember what each line of each codechunk does? Are there any codechunks that you struggle to make sense of? Make sure to revisit the section in which it is used and take notes.\n\n\n\n\nassum_tib &lt;- assum_tib |&gt; \n  dplyr::mutate( \n    assum_check = factor( \n      assum_check, \n      levels = c(\"No checks\", \"Insufficient\", \"No follow-up\", \"Full checks\"), \n      ordered = TRUE \n    ), \n    seniority = factor(seniority, levels = c(\"Faculty\", \"PostDoc\", \"PhD\")), \n    teach = factor(teach, levels = c(\"Yes\", \"No\")) \n  )\n\n\nassum_desc_teach &lt;- assum_tib |&gt; \n  dplyr::count(teach, assum_check) |&gt;  \n  dplyr::mutate( \n    percent = n/sum(n)*100, \n    .by = teach \n  )\n\nassum_desc_teach |&gt; \n  display()\n\n\nggplot2::ggplot(data = assum_desc_teach, \n                aes(x = assum_check, y = percent, \n                    colour = teach, linetype = teach, group = teach)) + \n  geom_point(size = 2) + \n  geom_line() + \n  scale_colour_viridis_d(end = 0.9) + \n  coord_cartesian(ylim = c(0, 100)) + \n  theme_light()\n\n\nassum_ord &lt;- MASS::polr(assum_check ~ seniority + teach, data = assum_tib) \n\n\nassum_tib &lt;- assum_tib |&gt; \n  dplyr::mutate( \n    level_1_vs_levels_234 = dplyr::case_when( \n      assum_check %in% c(\"No checks\") ~ 0,  \n      assum_check %in% c(\"Insufficient\", \"No follow-up\", \"Full checks\") ~ 1\n    ), \n    levels_12_vs_levels_34 = dplyr::case_when( \n      assum_check %in% c(\"No checks\", \"Insufficient\") ~ 0,  \n      assum_check %in% c(\"No follow-up\", \"Full checks\") ~ 1\n    ), \n    levels_123_vs_level_4 = dplyr::case_when( \n      assum_check %in% c(\"No checks\", \"Insufficient\", \"No follow-up\") ~ 0,  \n      assum_check %in% c(\"Full checks\") ~ 1\n    )\n  )\n\n\nmodel_log_1 &lt;- glm(level_1_vs_levels_234 ~ seniority + teach, data = assum_tib)\nmodel_log_2 &lt;- glm(levels_12_vs_levels_34 ~ seniority + teach, data = assum_tib)\nmodel_log_3 &lt;- glm(levels_123_vs_level_4 ~ seniority + teach, data = assum_tib)\n\n\ncompare_models(model_log_1, model_log_2, model_log_3, exponentiate = TRUE) |&gt; \n  display(digits = 3)\n\n\nmodel_parameters(assum_ord, exponentiate = TRUE) |&gt; \n  knitr::kable(digits = 3) \n\n\nassum_emm &lt;- emmeans::emmeans( \n  assum_ord,  \n  specs = \"assum_check\",  \n  by = \"teach\",  \n  at = list(seniority = \"Faculty\"), \n  mode = \"prob\" \n)\n\nassum_emm |&gt; \n  knitr::kable(digits=3)\n\n\nassum_emm |&gt; \n  as.data.frame() |&gt;  \n  ggplot2::ggplot(data = _, aes(x = assum_check, \n                                y = prob, ymin = asymp.LCL, ymax = asymp.UCL, \n                                linetype = teach, group = teach)) + \n  geom_point(size = 2) + \n  geom_errorbar(width = 0.1) +  \n  geom_line() + \n  coord_cartesian(ylim = c(0, 1)) + \n  theme_light()\n\n\n\nWorksheet\nScenario:\n\nA local GP surgery has rolled out a new patient app and wishes to understand the users’ satisfaction with the service. The app anonymously monitors engagement and records:\n\nThe outcome of the interaction with the app, with the options “Booked an appointment”, “Prescription request” or “Redirected to NHS information website”. [outcome]\n\n\n\nWhether the patient interacted with an AI chatbot while they used the app [ai_chatbot].\n\nWhen the user closes the app, they are presented with the following question:\n\nOn the whole, how would you rate your experience with this app today? where the patients can click on “Dissatisfied”, “Neutral” and “Satisfied” [user_satisfaction].\n\n\n\n\n\n\n\n\nUse the tutorial to complete the following tasks:\n\nGenerate descriptive statistics to summarise your data.\nFit a model predicting user satisfaction from both predictors (the outcome of the interaction). Use “Redirected” as the baseline for outcome and “Yes” as the baseline for ai_chatbot.\nCheck the model fit, including all the assumptions and fit statistics.\nGenerate parameter estimates and predicted probabilities.\nWrite up a short report (up to 300 words) summarising and interpreting the results of your analysis.\n\n\n\n\nYou can use the “worksheet” file in the quarto folder to prepare the worksheet. The data are stored in the file app_data.csv. Download the data and import it to Posit Cloud. Once you’ve done so, you can read the data into R by running:\n\napp_tib &lt;- here::here(\"data/app_data.csv\") |&gt; readr::read_csv()\n\nRemember to load the necessary packages.\n\n\n\n\n\n\nOptional extra task:\nIf you created a plot of predicted probabilities, you may have noticed that the lines and error bars overlap quite a bit, which makes it difficult to see. Can you think of a way to adjust the plot so that you can clearly see the extent to which the error bars overlap (or don’t overlap) with each other?\n\n\n\n\n\nCheck worksheet values\nOnce you’ve finished the worksheet, you can ask me to look through your work and give you feedback. Remember that you should also practice writing up the results in a brief report, not just running the code. If you’re stuck, you can use the quiz below to guide you.\nYou can also use the quiz below - if you fitted the models the correctly, your answers should match the values below.\n\n\n\n\n\n\nWorksheet check\n\n\n\n\n\nProportional odds check:\n\nai_chatbot estimate for Level 1 vs 2&3: 1.118\nai_chatbot estimate for Level 1&2 vs 3: 1.07\noutcomeBooked an appointment estimate for Level 1 vs 2&3: 1.115\noutcomeBooked an appointment estimate for Level 1&2 vs 3: 1.007\n\nThe proportional odds assumption was satisfied:\n\n TRUE FALSE\n\nFit statistics:\n\nDeviance explained: 13.4 %\nLikelihood ratio test statistic: 11.48\n\nParameter estimates:\n\nDissatisfied|Neutral : 0.194\nNeutral|Satisfied : 58.724\noutcomePrescription request: 0.749\noutcomeBooked an appointment: 2.613\nai_chatbotNo estimate: 4.359\n\nBased on the odds ratio alone (ignoring the p-value), patients who were redirected to an NHS website had higher odds of being more satisfied compared to patients who requested a prescription:\n\n TRUE FALSE\n\nBased on the odds ratio alone, patients who were redirected to an NHS website had higher odds of being more satisfied compared to patients who booked an appointment:\n\n TRUE FALSE\n\nBased on the odds ratio alone, patients who interacted with an AI chatbot had higher odds of being more satisfied compared to patients did not interact with the chatbot.\n\n TRUE FALSE\n\nEstimated probabilities\n\nProbability of “Neutral” satisfaction when interacting with the chatbot (holding outcome at the baseline): 0.888\nProbability of “Neutral” satisfaction when requesting prescription (holding ai_chatbot at the baseline): 0.781\n\n\n\n\n\n\n\n\nBürkner, Paul-Christian, and Matti Vuorre. 2019. “Ordinal Regression Models in Psychology: A Tutorial.” Advances in Methods and Practices in Psychological Science 2 (1): 77–101. https://doi.org/10.1177/2515245918823199.\n\n\nSladekova, Martina, Veronika L. Poupa, and Andy P. Field. 2024. “Sources of Bias in General Linear Models: Evaluating the Analytic Practice in Psychological Research.” https://doi.org/doi.org/10.31234/osf.io/wc42b.",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01c_gzlms_ordinal.html#footnotes",
    "href": "01c_gzlms_ordinal.html#footnotes",
    "title": "3  Modelling ordinal outcomes",
    "section": "",
    "text": "We’ll learn whether summing/averaging Likert scales like this is a good idea in the first place in Weeks 7, 8 and 9. Or we won’t - I’m not the one running the sessions, so it’ll be a surprise.↩︎\nIt’s not close by any means. But it is the closest.↩︎\nNo dig intended at PostDocs or PhDs. In the study cited above, we found differences in this general direction, but the overlap of uncertainty intervals was massive. Generally, the only group that tended to perform better where stats instructors.↩︎",
    "crumbs": [
      "Generalised Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelling ordinal outcomes</span>"
    ]
  },
  {
    "objectID": "01d_gzlms_summary.html",
    "href": "01d_gzlms_summary.html",
    "title": "Summary",
    "section": "",
    "text": "Canonical link functions\n\n\n\n\n\n\n\n\n\nDistribution\nUseful for…\nCanonical link function\nInverse function\n\n\n\n\nGamma\nRight skewed distributions\nlog\nexp",
    "crumbs": [
      "Generalised Linear Models",
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bürkner, Paul-Christian, and Matti Vuorre. 2019. “Ordinal\nRegression Models in Psychology: A Tutorial.” Advances in\nMethods and Practices in Psychological Science 2 (1): 77–101. https://doi.org/10.1177/2515245918823199.\n\n\nSladekova, Martina, Veronika L. Poupa, and Andy P. Field. 2024.\n“Sources of Bias in General Linear Models: Evaluating the Analytic\nPractice in Psychological Research.” https://doi.org/doi.org/10.31234/osf.io/wc42b.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "01_gzlms.html",
    "href": "01_gzlms.html",
    "title": "Generalised Linear Models",
    "section": "",
    "text": "Scenario",
    "crumbs": [
      "Generalised Linear Models"
    ]
  },
  {
    "objectID": "01_gzlms.html#scenario-gzlms",
    "href": "01_gzlms.html#scenario-gzlms",
    "title": "Generalised Linear Models",
    "section": "",
    "text": "A researcher in sports psychology is interested in understanding the factors related to the reaction times of belayers1 when breaking a fall of their climbing partners. She set up ultra-high resolution cameras at the local gym, then reviews the footage and records: the time it took for the belayer to effectively stop the fall after the climber lost control in milliseconds (reaction_time); whether the belayer used an assisted-breaking device (assisted_break - no/yes); and whether the belayer was talking to others or was otherwise distracted (distracted - no/yes). The researcher also collected information about the participants’ climbing experience in years (climbing_exp), and the length of the climbers’ fall in metres (fall_m).\nThe researcher hypothesises the following:\nH1: More experienced belayers will be faster to catch falls.\nH2: Belayers using an assisted breaking device will be faster to catch falls.\n\n\nThe data\nThe (fictional) dataset is stored in the file climbing_data.csv . First, let’s load up the necessary packages. Then, assuming you have imported the dataset into the data folder on Posit Cloud, you can read it into your R session as:\n\n1library(easystats)\n2library(tidyverse)\n\n3belay_tib &lt;- here::here(\"data/climbing_data.csv\") |&gt;\n4  readr::read_csv()\n\n\n1\n\nLoad the easystats package\n\n2\n\nLoad the tidyverse package\n\n3\n\nSpecify the directory of the dataset with the here function.\n\n4\n\nRead the data, storing the result of this code into an object called belay_tib.\n\n\n\n\nLet’s inspect the dataset to make sure it read in properly:\n\nbelay_tib\n\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\n\n\nWe have six variables here: the ID of the participant subj_id , the outcome reaction_time and four predictors as specified in the Scenario above. assisted_break and distracted are currently encoded as 0s and 1s, 0 standing for “No” and 1 standing for “Yes”.\n\n\n\n\n\n\nFactors and binary categorical predictors\n\n\n\n\n\nIdeally, categorical predictors should be encoded as factors with meaningful labels. However if a categorical predictor with two categories is coded as 0 and 1, we don’t strictly need to convert them into factors. Zeros and ones are exactly what we use when dummy coding, so we can use these variables as they are to fit a model. R will cope just fine and the statistics will be valid. The output will just not be as helpfully labelled, so we need to remember that the beta for these predictors will tell is how larger or smaller the estimate of the category coded as 1 is compared to the category coded as 0.\nSituations where we absolutely have to convert into factors or out right dummy codes include: when the nummeric categories are different to 0 and 1 (e.g. they’re 1 and 2, or 2 and 3 for whatever reason) or when there are more than two categories. Especially in the latter case, it makes no sense to treat a categorical predictor with three categories as if it was continuous.\n\n\n\n\n\nDescriptive statistics\nThe next few steps should be familiar. We start by generating some descriptive summaries:\n\n1belay_tib |&gt;\n2  describe_distribution(select = -subj_id) |&gt;\n3  display()\n\n\n1\n\nTake the object belay_tib -\n\n2\n\nPipe it into describe_distribution() which will provide general descriptive summaries. We’re including some optional arguments:\n\n3\n\nApply formatting to create a nicer looking table (optional, but useful for e.g. writing reports)\n\n\n\n\n\n\nTable 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nRange\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nreaction_time\n523.14\n141.68\n172.73\n(316.12, 1030.44)\n1.21\n1.79\n152\n0\n\n\nassisted_break\n0.52\n0.50\n1.00\n(0.00, 1.00)\n-0.08\n-2.02\n152\n0\n\n\ndistracted\n0.49\n0.50\n1.00\n(0.00, 1.00)\n0.03\n-2.03\n152\n0\n\n\nclimbing_exp\n6.14\n1.48\n1.97\n(2.10, 9.70)\n-0.22\n0.28\n152\n0\n\n\nfall_m\n1.82\n0.41\n0.60\n(0.90, 3.00)\n0.19\n0.06\n152\n0\n\n\n\n\n\n\n\n\n\nWe have no missing values, for any of the variables (good to know). Reaction time (RT) is in millisecond so the average reaction time for stopping a fall was a little over half a second (523.14 ms). The quickest belayer caught a fall in 316 ms and the slowest in 1030 ms. On average, belayers had a climbing experience of 6.14 years, ranging from 2.1 to 9.7 years. Average fall length was 1.82 metres. The smallest recorded fall was 0.9 metres, the longest was 3 metres.We’re not interested in central tendency, dispersion, or skewness and kurtosis for the assisted_break and distracted (it makes no sense to inspect these for categorical predictors).\nClimbing experience and fall distance show low levels of skewness and kurtosis. However, our outcome variable is positively skewed (skewness = 1.21), and has excess kurtosis substantially above 0 (kurtosis = 1.79).\nThis is not that surprising. Reaction times are bound at 0 (you cannot have a negative reaction time) and unbound on the opposite tail, so it really doesn’t make sense to expect a normal distribution, which is unbound on both ends. Even if we don’t have this intuition, reaction time is the best documented and most stereotypical variable known to have a skewed distribution. Despite this, researchers often model it with estimators that rely on normal distribution (somewhere in the process), simply because it’s convenient. So for now, we’re going to assume ignorance to see whether we can pry something meaningful out of the model using a regular linear model.\n\n\n\n\n\n\nNormal distribution of the outcome\n\n\n\n\n\nRemember that when we’re fitting an OLS-based linear model, the normal distribution of the outcome variable is not one of the assumptions. The actual assumption refers to the errors in the population model, which are best checked by inspecting the model residuals. However, the outcome variable will often (but not always) have a similar distribution to the residuals. It is therefore useful to inspect it in the initial stage, but we’ll still need to run the residual diagnostics after we’ve fitted the model.\n\n\n\n\n\nVisualisation\nFor data visualisation, we’ll want:\n\nA plot showing the hypothesised relationships between each predictor and the outcome.\nA plot showing the distribution of the outcome, just so we have a better sense of what the variable looks like\n\nLet’s start with H1:\n\nbelay_tib |&gt; \n1  ggplot2::ggplot(data = _, aes(x = climbing_exp, y = reaction_time)) +\n2  geom_point(alpha = 0.7, colour = \"yellowgreen\") +\n3  stat_smooth(method = \"lm\", colour = \"steelblue\", fill = \"steelblue\") +\n4  coord_cartesian(xlim = c(0, 10), ylim = c(0, 1030.5)) +\n5  labs(x = \"Belayer's climbing experience (in years)\", y = \"Reaction time (ms) to break the fall\") +\n6  theme_light()\n\n\n1\n\nSpecify the dataset and the aesthetics\n\n2\n\nAdd raw data points\n\n3\n\nAdd the line of best fit\n\n4\n\nScale the y axis and the x axis to reflect the realistic scale of our outcome variable (the maximum value 1030.5 is based on values from Table 2)\n\n5\n\nAdd meaningful labels\n\n6\n\nAdd a theme (optional but generally a good idea)\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\n\nLearning about ggplots\n\n\n\n\n\nggplot is a fantastic tool but the code is objectively difficult to remember - it’s chonky, and each line has far too many options for customisation. When working with ggplot code, don’t just copy and paste the whole code chunk and call it a day - instead, write the first line, run it, see what it does. Then write the second line, add it to the plot, and see what changes. And so on. This goes for any code that’s longer than just a couple of lines. By writing it line by line, you’ll get a better understanding of what the code does, which bits are crucial and which ones are just “nice to have”. For example, I didn’t need to change the colour of the points in line 3, nor did I need to add the jitter, or the labels for the x and y axis. Without these, the plot would still be understandable. But adding these elements the plot more readable and accessible.\n\n\n\n\nNow let’s do the same for H2\n\nbelay_tib |&gt; \n1  ggplot2::ggplot(data = _, aes(x = factor(assisted_break), y = reaction_time)) +\n2  geom_point(alpha = 0.7, colour = \"yellowgreen\", position = position_jitter(width = 0.1)) +\n3  stat_summary(fun.data = \"mean_cl_normal\") +\n4  coord_cartesian(ylim = c(0, 1030.5)) +\n5  labs(x = \"Did the belayer used an assisted breaking device\", y = \"Reaction time (ms) to break the fall\") +\n6  theme_light()\n\n\n1\n\nSpecify the dataset and which variable to put on each of the axes. Note that we’re wrapping assisted_break in the factor() function, so the plot treats it as such. This is optional. Again, it would have been better to convert the variable to the factor at the very beginning, but we’re keeping it as numeric for now because it makes some of the later calculations easier.\n\n2\n\nAdd raw data points - this is good for inspecting potentially unusual cases\n\n3\n\nAdd the mean and confidence intervals - this time, the predictor is categorical so we use stat_summary instead of stat_smooth\n\n4\n\nScale the y axis\n\n5\n\nAdd meaningful labels\n\n6\n\nAdd a theme\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nFinally, let’s have a look at the distribution of the outcome:\n\nbelay_tib |&gt; \n  ggplot2::ggplot(data = _, aes(x = reaction_time)) + \n  geom_density(colour = \"steelblue\", fill = \"steelblue\", alpha = 0.1) + \n  labs(x = \"Reaction time (ms) to break the fall\", y = \"Density\") + \n  theme_light()\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nNo code description for this one - see if you can work it out!\n\nFrom Figure 1, we see that there is quite a steep negative relationship between climbing experience of the belayer and reaction time - the more experience the belayer is, the shorter they take to respond. At a glance, it also seems like the belayers who used an assisted-breaking device were slightly faster to react compared to those who didn’t (Figure 2).\nFinally, Figure 3 confirms what Table 2 told us - that the outcome variable is right-skewed, with most values clustered towards the lower end an asymmetrical right tail. Without further ado, let’s confidently ignore this fact and hope for the best when fit an OLS linear model.",
    "crumbs": [
      "Generalised Linear Models"
    ]
  },
  {
    "objectID": "01_gzlms.html#ordinary-least-squares-estimation",
    "href": "01_gzlms.html#ordinary-least-squares-estimation",
    "title": "Generalised Linear Models",
    "section": "Ordinary Least Squares estimation",
    "text": "Ordinary Least Squares estimation\n\nFit the model\nWe can fit the linear model with all four predictors as:\n\nbelay_lm &lt;- lm(reaction_time ~ assisted_break + climbing_exp + distracted + fall_m, data = belay_tib)\n\nBefore diving straight into the result, we’ll need to check the model assumptions and assess the model fit.\n\n\nCheck model assumptions\n\nbelay_lm |&gt; check_model()\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nRight of the bat, we have problems. The posterior predictive check (row 1 left) shows that the distribution of values predicted by the model doesn’t align with the distribution observed in the dataset. There appears to be slight non-linearity, and the spread of the residuals is greater as we move right along the x axis (row 1 right) - this indicates the errors are heteroscedastic. This is confirmed by the fact that the line is not completely flat in row 2 left plot. There don’t appear to be any influential observations, but case 19 has Cook’s distance that is close to the cut-off of 0.8 (row 2 right). No particular issues with collinearity (row 3 left). Finally, the Q-Q plot in row 3 right confirms what we suspect based on the outcome distribution - that the distribution of the residuals is skewed with potentially extreme values, as evident by the right tail lifting above the horizontal reference line.\n\n\nCheck model fit\nWe can get the overall fit statistics:\n\nbelay_lm |&gt; \n  performance() |&gt; \n  display()\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n\n\n1848.0\n1848.6\n1866.1\n0.48\n0.47\n101.54\n103.26\n\n\n\n\n\n\nAnd test the overall fit:\n\nbelay_lm |&gt; \n  test_wald() |&gt; \n  display()\n\n\n\n\nName\nModel\ndf\ndf_diff\nF\np\n\n\n\n\nNull model\nlm\n151\n\n\n\n\n\nFull model\nlm\n147\n4\n34.33\n&lt; .001\n\n\n\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\n\nThe model appears to be a good fit and a statistically significant improvement over a model where the reaction time is predicted from intercept alone, F(4,147) = 34.33, p &lt; .001 , explaining 48.3% of total variance, R2 = 0.48 , R2adj = 0.47 . However, before we congratulate ourselves, let’s remember that the model assumptions were violated in almost every respect, so we might want to address this in some way.",
    "crumbs": [
      "Generalised Linear Models"
    ]
  },
  {
    "objectID": "01_gzlms.html#robust-estimation",
    "href": "01_gzlms.html#robust-estimation",
    "title": "Generalised Linear Models",
    "section": "Robust estimation",
    "text": "Robust estimation\nRobust estimation is one approach that can be used for dealing with violated assumptions - instead of interpreting the parameters from the OLS model, we fit a robust model that is not sensitive (or less sensitive) to violated assumptions. We then use the parameter estimates from this model to draw conclusions about the hypotheses.\nThere are many robust methods out there. To recap, some include:\n\nBootstrapping - useful in small samples when normal errors cannot be assumed\nHeteroscedasticity-consistent standard errors - useful when homoscedasticity cannot be assumed\nM-type estimators - useful when we have influential cases or asymmetric error distributions, but can also deal with heteroscedasticity.\n\nGiven that we’re dealing with asymmetric errors, heteroscedasticity, and potential influential cases (Figure 4), let’s fit an MM-estimator:\n\nbelay_rob &lt;- robustbase::lmrob(reaction_time ~ assisted_break + climbing_exp + distracted + fall_m, data = belay_tib)\n\nWe can now request the parameter estimates:\n\nbelay_rob |&gt; \n  parameters() |&gt; \n  display()\n\n\n\nTable 3\n\n\n\n\nFixed Effects\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(147)\np\n\n\n\n\n(Intercept)\n689.26\n53.94\n(582.66, 795.86)\n12.78\n&lt; .001\n\n\nassisted break\n-26.77\n14.04\n(-54.52, 0.98)\n-1.91\n0.059\n\n\nclimbing exp\n-53.49\n6.23\n(-65.79, -41.18)\n-8.59\n&lt; .001\n\n\ndistracted\n50.84\n15.48\n(20.26, 81.42)\n3.29\n0.001\n\n\nfall m\n76.16\n20.66\n(35.33, 116.99)\n3.69\n&lt; .001\n\n\n\n\n\n\n\n\n\nThere is a negative but non-significant difference in reaction time between belayers who used an assisted breaking device compared to those who didn’t, b = -26.77, 95% CI [-54.52, 0.98], SE = 14.04, t = -1.91, p = 0.059 - those using a breaking device were only -26.77 quicker. Based on this we cannot reject the null hypothesis of no difference between belaying devices.\nBelayers with more climbing experience were faster to stop a fall, while holding other predictors constant, b = -53.49, 95% CI [-65.79, -41.18], SE = 6.23, t = -8.59, p &lt; .001. With a one year increase in climbing experience, we can expect the reaction time to reduce by -53.49 . The 95% confidence interval suggest this reduction could be as large as -41.18 or as low as -65.79 , assuming this confidence interval is one the 95% that contain the true population value. If the null hypothesis were true, the probability of finding an effect this large is sufficiently unlikely, p &lt; .001. We therefore reject the hypothesis of null effect and retain H2.\n\n\n\n\n\n\nWhat’s with this awful language…\n\n\n\n\n\nLook, if it helps you make sense of this, here’s a slightly more direct version:\n\nIt is unlikely that we would find this difference if the effect itself was in reality 0, p &lt; .001, which is in support of H2.\n\nHowever, it is technically and statistically incorrect . A statistically significant p-value can never provide support for an alternative hypothesis, no matter how small. It only tells us if the alternative hypothesis is sufficiently unlikely if the null is true, but it doesn’t tell the probability of the null or the alternative hypothesis being true. Bayes Factors can tell us that, but that’s a whole other module.\n\n\n\nThe effects of both covariates were also statistically significant. Distracted belayers were slower to react b = 50.84, 95% CI [20.26, 81.42], SE = 15.48, t = 3.29, p = 0.001 , while an increase in 1 metre of fall length was associated with an increase of 76.16 milliseconds in time taken to stop the fall (p &lt; .001). This is not surprising, is the climber had a greater distance to fall before the rope ran out of slack.",
    "crumbs": [
      "Generalised Linear Models"
    ]
  },
  {
    "objectID": "01_gzlms.html#making-predictions",
    "href": "01_gzlms.html#making-predictions",
    "title": "Generalised Linear Models",
    "section": "Making predictions",
    "text": "Making predictions\nNow that we’ve caught up, let’s push our models even further. Generally, there are two reasons why we might want to build a statistical model. (1) To test a theory and help build a theoretical framework by explaining the roles of key variables or (2) to make a prediction. These two are not mutually exclusive.\nRecall that a linear model can be describe as:\n\\[\nY_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... + \\beta_nX_{ni} + \\varepsilon_i\n\\]\nThat is, the outcome \\(Y\\) for the participant \\(i\\) is predicted from the sum of the intercept \\(\\beta_0\\) a value of each predictor for that participant (\\(X_1, X_2...\\)), multiplied by the parameter estimate for each predictor, respectively (\\(\\beta_1 , \\beta_2 ...\\)). Each participant will also have their own error in prediction \\(\\varepsilon_i\\) .\n\nPredictions within the observed data\nThe equation specific to our model is:\n\\[\n\\begin{split}\n\\mbox{reaction time}_i = \\beta_0 &+ \\beta_1\\mbox{assisted break}_{i} \\\\\n&+ \\beta_2\\mbox{climbing experience}_{i} \\\\\n&+ \\beta_3\\mbox{distracted}_{i} \\\\\n&+ \\beta_4\\mbox{fall distance}_{i} \\\\\n& + \\varepsilon_i\n\\end{split}\n\\]\nWe’ve worked out the beta values, so let’s plug them in from Table 3 :\n\\[\n\\begin{split}\n\\mbox{reaction time}_i = 689 &-26.77*\\mbox{assisted break}_{i} \\\\\n&- 53.49*\\mbox{climbing experience}_{i} \\\\\n&+ 50.84*\\mbox{distracted}_{i} \\\\\n&+ 76.16*\\mbox{fall distance}_{i} \\\\\n&+ \\varepsilon_i\n\\end{split}\n\\]\nLet’s test this - how about we use the predictor values from the first participant from our dataset in Table 1 to see if we can use our model to predict their outcome?\nHere are their values:\n\nbelay_tib[1, ] |&gt; \n  display()\n\n\n\n\n\n\n\n\n\n\n\n\nsubj_id\nreaction_time\nassisted_break\ndistracted\nclimbing_exp\nfall_m\n\n\n\n\n1017\n847.40\n0\n1\n3.10\n2.10\n\n\n\n\n\n\nWe can assign these values into objects and then rewrite the equation.\n\nassisted_break_i &lt;- 0 \nclimbing_exp_i &lt;- 3.1\ndistracted_i &lt;- 1\nfall_m_i &lt;- 2.1 \n\n689.29 - 26.77 * assisted_break_i - 53.49 * climbing_exp_i + 50.84 * distracted_i + 76.16 * fall_m_i\n\n[1] 734.247\n\n\nIf we run the code chunk above, the model predicts that the first participant should have a reaction time of milliseconds. However, their actual reaction was 847.4. This is because we forgot the last part of the equation, \\(\\varepsilon_i\\). This represents the residual for our participant. We can extract it from the model and add it to the equation:\n\nresidual_i &lt;- belay_rob$residuals[1] # extract the residual for the first participant\n\n689.29 - 26.77 * assisted_break_i - 53.49 * climbing_exp_i + 50.84 * distracted_i + 76.16 * fall_m_i + residual_i\n\n       1 \n847.4194 \n\n\nIn this case, the value matches perfectly, but this only works if we are predicting values that are already in the model, because we can extract the residual direct. In practice, we often want to predict values for individuals who are not in the dataset - in such a case we accept we’ll often be slightly off because we don’t know how large the residual could be, but our prediction is our best guess that will result in the least possible error.\n\n\n\n\n\n\nIs there an easier way…?\n\n\n\n\n\nYes. R automatically computes the values your model predicts for every single participant. We did this manually above using the equation, but we could have just as easily done it with code. Fitted values can be accessed as:\n\nbelay_rob$fitted.values\n\nThis is a list of values that corresponds to the rows in our dataset. So the first value corresponds to the predicted value of reaction for the first participant:\n\nbelay_rob$fitted.values[1]\n\n       1 \n734.2276 \n\n\nThis is exactly the value we calculated manually, so it’s good to know our maths works!\n\n\n\n\n\nGoing beyond the data\nThe nice thing about modelling relationships as straight lines is that once we know the intercept and the slope, we can pretty much extend the line in either direction and make predictions for values that are not represented in your dataset. Consider Figure 1 . Although the line only spans the range of the data, we could extend each end to make predictions.\nSay that we’re interested in what the model predicts for individuals with no climbing experience, or, on the other hand, with extensive climbing experience. We’re going to create a new tibble with the values of climbing_exp for which we want to predict, that is, for belayers with anywhere between 0 and 20 years of experience.\nThen we’ll hold the other predictors constant. For factors, we’ll hold the values at the baseline, so 0 or “No” for both assisted_break and distracted . The continuous predictor - fall length - will be held constant at the average value of fall_m from Table 2 . We’ll use the tibble function and store the result into an object called prediction_tib .\n\nprediction_tib &lt;- tibble::tibble(\n  climbing_exp = 0:20,\n  assisted_break = 0,\n  distracted = 0,\n  fall_m = 1.82\n)\n\nInspect the tibble:\n\nprediction_tib\n\n\n\n\n\n\n\n\nAs requested, values in all rows are identical except for climbing_exp, which is changing. Now we can use this table to make predictions. The predict function is very handy here: it only takes two arguments: the model and the tibble with values which we want to use for prediction.\nOn its own, it can be used like this:\n\npredict(belay_rob, prediction_tib)\n\nWhich returns a vector of predicted values. Let’s be a little more efficient - we’re going to take the tibble we’ve just created - prediction_tib and use the mutate function to create a new column called predicted_rt . This column is created by applying the predict function:\n\n1prediction_tib &lt;- prediction_tib |&gt;\n2  dplyr::mutate(\n3    predicted_rt = predict(belay_rob, prediction_tib)\n  )\n\n\n1\n\nTake the existing prediction_tib. We’re updating an existing object instead of creating a new one, so the assignment operator &lt;- is also pointing towards prediction_tib.\n\n2\n\nUse the mutate function to create a new variable.\n\n3\n\nCall this new variable predicted_rt. Create predicted_rt by applying the predict function to the robust model belay_rob and the prediction_tib which contains all the values for which we’re trying to generate predictions.\n\n\n\n\n\nOnce again, let’s inspect our work:\n\nprediction_tib\n\n\n\n\n\n\n\n\nThis time the tibble has an extra column with predicted values. If you inspected the predicted_rt values, you may already noticed that something is wrong. Let’s add these on a plot so we can see the prediction more clearly:\n\nbelay_tib |&gt; \n  ggplot2::ggplot(data = _, aes(x = climbing_exp, y = reaction_time)) + \n  geom_point(alpha = 0.7, colour = \"yellowgreen\") + \n  stat_smooth(colour = \"darkred\", data = prediction_tib, aes(x = climbing_exp, y = predicted_rt)) +\n  labs(x = \"Belayer's climbing experience (in years)\", y = \"Reaction time (ms) to break the fall\") + \n  theme_light() \n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nWe start the scatterplot above as usual. Then we add stat_smooth but instruct ggplot to use a different dataset and different aesthetics than the one we provided when we initialised the plot, so we can show our prediction that goes beyond the data.\nIf we look at what the model predicts for belayers with no climbing experience, it’s not particularly unusual. The reaction time is longer, though notice there are four individuals in the top left above the line who have more experience but also seem to have longer reaction times than newbies. The model doesn’t do a very good job predicting values for these belayers.\nNow let’s look at very experienced belayers. The reaction continues going down, however as we get to about 15 years of experience, the model starts predicting negative reaction times.\nThis, of course, makes no sense. It turns out this the convenient feature of being able to easily extend the line once we know it’s starting point and the direction is also the downfall of linear models - the line can go on forever and will inevitably predict nonsense if the linear model is not the appropriate model.\nRobust models are also - if not more - susceptible. They down-weigh scores with high residuals, especially scores at the tails of asymmetric distributions. The robust parameter estimate is therefore very good at describing what is happening at the centre of the distribution and make predictions for these scores, but it cannot describe or make predictions for individuals at the tails. That doesn’t mean robust models are useless. We just need be clear about what our intentions are and what we’re hoping to get out of the model. If the goal is to summarise the “typical individual” and we don’t particularly care about what’s happening at the tails, robust models might be just the solution we need.\nIf, however, we hope to be able to make conclusions about the whole distribution, we must take a different approach.",
    "crumbs": [
      "Generalised Linear Models"
    ]
  },
  {
    "objectID": "01_gzlms.html#introduction-to-gzlm",
    "href": "01_gzlms.html#introduction-to-gzlm",
    "title": "Generalised Linear Models",
    "section": "Introduction to GzLM",
    "text": "Introduction to GzLM\nGzLM are useful and versatile, but we shouldn’t just think about them as a “fix” for GLM models that don’t meet the assumptions. When it comes to data analysis that involves confirmatory hypothesis testing with p-values, it’s generally a bad idea to base key analytic decisions on something we’ve discovered in the data because doing so invalidates the p-values derived from our models. In other words, a good analytic pipeline should not take the form of (1) fit an OLS model (2) discover violated assumptions (3) fit GzLM to fix the problems.\nInstead, we should think about the data generating process when we are planning the analyses, or, better yet, before we even start data collection. Considering the variables we’re aiming to collect, what might the distributions look like? Is normal distribution a reasonable expectation? Is variance that doesn’t change at different values of the predictors a reasonable expectation? More often than not, the answer to the latter two questions is no.\n\nHow is GzLM different to GLM?\nGLM uses Ordinary Least Squares (OLS) estimation to fit models. The goal of OLS is to find the values of \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) … so that the sum of squared errors (SSE) is as small is it can be given the data provided. Consider Figure 1 : if we were to measure the distance of each point from the line (i.e. the residual), square each distance and then sum all of them together, we’d get an SSE. For the model depicted in Figure 1, the line is the most optimal line that gives the smallest possible SSE.\nThe process is a little different for robust models using M-type estimation wherein the cases with the largest residuals get itteratively down-weighted until an optimal line is found, but the starting point is the result of OLS estimation.\n\nMaximum Likelihood estimation\nGzLM use Maximum Likelihood Estimation (ML)2. ML aims to find an optimal way to fit a given distribution to the data. It searches for the values of the the parameter estimates where a given assumed distribution is the most likely. In other words, where the likelihood of that distribution is at its maximum.\n\n\nConditional distributions\nWhat are we assuming the distribution of?\nWe know that OLS makes assumptions about the shape of errors in the population model - that is, it assumes they are normally distributed. Various guides will sometimes claim that we need to be checking a so called “response distribution”, which is just another word for the outcome. This is not quite right.\nML estimation makes assumptions about the conditional distribution of the outcome. Say that we’re trying to model the relationship between X and Y where\n\\[\nY_i = \\beta_0 + \\beta_1 X_i\n\\]\nIf we model this as a Gaussian model - that is, a model assuming a normal conditional distribution - we’re assuming that that the outcome Y is normally distribution at any value of X. As a formula, we would write:\n\\[\nP(Y|X) \\sim N(\\mu, \\sigma^2)\n\\]\nThat is, any Y value given any X values comes from a normal distribution with a mean \\(\\mu\\) and variance \\(\\sigma^2\\) .\nThe nice thing about GzLMs is that we can just relax this assumption if a different distribution is more appropriate.\nFor example here:\n\\[\nP(Y|X) \\sim \\Gamma(\\alpha, \\theta)\n\\]\nwe’re saying that the conditional distribution of Y given X is a Gamma distribution with a shape \\(\\alpha\\) and scale \\(\\theta\\) . More on this in section Modelling Skewness.\nThis distinction between the distribution of the outcome variable and the conditional distribution of the outcome variable is important. Similar to OLS, if we check the distribution of the outcome, it often will resemble the conditional distribution but this not a given - the outcome distribution will be composed of the many conditional distributions at various levels of the predictor(s) and there is no guarantee that the shape of all of these distributions will resemble the one we’re modelling when combined into the outcome variable.\nHave a look at Figure 6 . Hopefully you’ll agree that this is not a normal distribution:\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\nHowever, if we were do decompose it, we’d find that the distribution in Figure 6 is actually built out of many normal distributions with different means:\n\n\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\nTherefore checking the distribution of the outcome itself might be a good starting point, but it can lead us astray, especially if the conditional distribution substantially varies in parameters.\n\n\nMean-variance relationship\nIn GLM, we assume a constant variance of errors. That is, regardless of the predicted value, the spread of errors remains the same - this is homoscedasticity. Its violation, as we saw in Figure 4 is called heteroscedasticity.\nM-type robust estimators attempt to fix the impact of heteroscedasticity by (1) down-weighting cases with extreme residuals and (2) using an alternative estimator for the standard errors. Either way, heteroscedasticity is seen as some nuisance to be fixed.\nOn the other hand, GzLM can model various forms of heteroscedasticty explicitly and build these changes in variance into predictions. Different conditional distributions make different assumptions about the mean-variance relationship. For example, Gamma models assume a quadratic relationship:\n\\[\nV(Y) = E(Y)^2\n\\]\ni.e. in a given conditional distribution, the variance of scores will be equal to the squared expected3 value. Alternatively, the Poisson distribution can model a situation where the expected value is equal to the variance.\n\\[\nV(Y) = E(Y)\n\\]\nWe’ll cover these situations in more detail in the upcoming weeks. The bottom line is that GzLM offers a lot more flexibility than GLM, which can be useful in a variety of applied scenarios. So before we get too bogged down in terminology, let’s fit one more bad model to get the hang of the coding, and then we’ll finally move on to modelling tackling out hypotheses properly by choosing a more appropriate modelling approach.\n\n\n\nFitting GzLM in R\nSo let’s fit our model. We’re still going to (wrongly) assume the normal conditional distribution for our outcome, but this time we’ll fit the model using maximum likelihood estimation instead of OLS.\n\n1belay_glm &lt;- glm(\n2  reaction_time ~ assisted_break + climbing_exp + distracted + fall_m,\n3  family = gaussian(link = \"identity\"),\n4  data = belay_tib\n)\n\n\n1\n\nUse the glm() function instead of lm() . Store into an object belay_glm\n\n2\n\nSpecify the formula - same as before.\n\n3\n\nSpecify the family and the link function (see below)\n\n4\n\nSpecify the dataset - same as before\n\n\n\n\nThe main difference from the OLS model or the robust model is this line: family = gaussian(link = \"identity\") . Here, we’re telling R to assume normal conditional distribution (i.e. gaussian). We’re also asking R not to transform the variables in any way and model the relationship as linear directly (`link = \"identity\"). We’ll talk about what exactly this means in the upcoming sections but for now you just need to understand that the link = \"identity\" ensures that the parameter estimates we get out of the model can be interpreted as we’re used to, as a linear increase in the outcome expected with an increase of one unit in the predictor.\nWe can run the model checks as normal - the output is the same as before and can be interpreted as such, so there’s no need to repeat it here.\n\nbelay_glm |&gt; \n1  check_model()\n\nbelay_glm |&gt; \n2  performance()\n\nbelay_glm |&gt; \n3  test_wald()\n\n\n1\n\nCheck model assumptions\n\n2\n\nOverall fit statistics\n\n3\n\nTest of model fit\n\n\n\n\nThe parameter estimates will also be identical. The ML estimation is an algorithmic process with several steps, in which the model tries out different values before converging on ones with the maximum likelihood given the data. However, with Gaussian models, this algorithm converges on the exact same parameter estimates as OLS right after the first step.\nWe’ve not actually produced the OLS estimates before, so let’s compare them to the ML estimates side by side:\n\nOLS (GLM) estimatesML (GzLM) estimates\n\n\n\nbelay_lm |&gt; \n  parameters() |&gt; \n  display()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(147)\np\n\n\n\n\n(Intercept)\n732.93\n51.00\n(632.14, 833.73)\n14.37\n&lt; .001\n\n\nassisted break\n-33.03\n17.07\n(-66.77, 0.71)\n-1.93\n0.055\n\n\nclimbing exp\n-59.96\n5.80\n(-71.42, -48.49)\n-10.33\n&lt; .001\n\n\ndistracted\n70.29\n16.90\n(36.89, 103.69)\n4.16\n&lt; .001\n\n\nfall m\n77.54\n20.86\n(36.32, 118.77)\n3.72\n&lt; .001\n\n\n\n\n\n\n\n\nbelay_glm |&gt; \n  parameters() |&gt; \n  display()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(147)\np\n\n\n\n\n(Intercept)\n732.93\n51.00\n(632.97, 832.90)\n14.37\n&lt; .001\n\n\nassisted break\n-33.03\n17.07\n(-66.49, 0.43)\n-1.93\n0.053\n\n\nclimbing exp\n-59.96\n5.80\n(-71.33, -48.58)\n-10.33\n&lt; .001\n\n\ndistracted\n70.29\n16.90\n(37.17, 103.42)\n4.16\n&lt; .001\n\n\nfall m\n77.54\n20.86\n(36.65, 118.43)\n3.72\n&lt; .001\n\n\n\n\n\n\n\n\nAlthough the estimates themselves are the same, the confidence intervals and the p-values are not. This is deliberate. When we fit OLS models, we use the critical t statistic4 based on the residual degrees of freedom to construct the CIs. The test statistic is then calculated by dividing the parameter estimate by its standard error, and this is compared against a theoretical t distribution to calculate the p-value.\nWhen we fitted the GzLM, we explicitly told R to assume a normal/Gaussian distribution. So the model takes us for our word, and instead of using the t distribution, it compares the test statistic against a normal distribution. Likewise, the CIs are constructed using the critical Z value, not the critical t value. The parameters function is quite clever in that it automatically detects what kind of model we fitted and gives us the result based on that.",
    "crumbs": [
      "Generalised Linear Models"
    ]
  },
  {
    "objectID": "01_gzlms.html#conclusion",
    "href": "01_gzlms.html#conclusion",
    "title": "Generalised Linear Models",
    "section": "Conclusion",
    "text": "Conclusion\nOf course, if we were to actually run any of the model checks, we’d still find that the model is fundamentally flawed. Regardless of the approach, a normal distribution is not the appropriate distribution to be assuming here. If we fit a robust model, the predictions we’re generating are only useful for the middle of the distribution, while the predictions at the tails breach the realm of science-fiction with reality-bending reaction times. If we want to create a useful model of the reality, we need to change the assumptions we’re making about the data-generating process.\n\nWelcome to the family\nThe General Linear Model encompasses many types of models that use OLS estimation (like the t-test, ANOVA, regression and others). The Generalised Linear Model takes this even further and can model various forms of the Exponential Family . This may seem counter-intuitive because when we say “Exponential” we often imagine a line that grows or descends steeply (or “exponentially”), but this distribution family in an overarching group of distributions that come in many shapes and sizes, including:\n\nBernoulli distribution (often assumed in logistic regression)\nChi-squared distribution\nDirichlet distribution\nExponential distribution\nGamma distribution\nNormal distribution (yes, even normal distribution)\nPoisson distribution\n\nand others. We’re going to learn about some of these in the coming weeks.",
    "crumbs": [
      "Generalised Linear Models"
    ]
  },
  {
    "objectID": "01_gzlms.html#exercises",
    "href": "01_gzlms.html#exercises",
    "title": "Generalised Linear Models",
    "section": "Exercises",
    "text": "Exercises\n\nWhat does this code do?\n\n\n\n\n\n\nHere’s all the code we have written in this section. Can you remember what each line of each codechunk does? Are there any codechunks that you struggle to make sense of? Make sure to revisit the section in which it is used and take notes.\n\n\n\n\nlibrary(easystats) \nlibrary(tidyverse) \n\nbelay_tib &lt;- here::here(\"data/climbing_data.csv\") |&gt; \n  readr::read_csv() \n\n\nbelay_tib |&gt; \n  describe_distribution(select = -subj_id) |&gt; \n  display() \n\n\nbelay_tib |&gt; \n  ggplot2::ggplot(data = _, aes(x = climbing_exp, y = reaction_time)) + \n  geom_point(alpha = 0.7, colour = \"yellowgreen\") + \n  stat_smooth(method = \"lm\", colour = \"steelblue\", fill = \"steelblue\") + \n  coord_cartesian(xlim = c(0, 10), ylim = c(0, 1030.5)) + \n  labs(x = \"Belayer's climbing experience (in years)\", y = \"Reaction time (ms) to break the fall\") + \n  theme_light() \n\n\nbelay_tib |&gt; \n  ggplot2::ggplot(data = _, aes(x = reaction_time)) + \n  geom_density(colour = \"steelblue\", fill = \"steelblue\", alpha = 0.1) + \n  labs(x = \"Reaction time (ms) to break the fall\", y = \"Density\") + \n  theme_light()\n\n\nbelay_lm |&gt; check_model()\n\n\nbelay_lm |&gt; \n  performance() |&gt; \n  display()\n\n\nbelay_lm |&gt; \n  test_wald() |&gt; \n  display()\n\n\nbelay_rob |&gt; \n  parameters() |&gt; \n  display()\n\n\nprediction_tib &lt;- tibble::tibble(\n  climbing_exp = 0:20,\n  assisted_break = 0,\n  distracted = 0,\n  fall_m = 1.82\n)\n\n\npredict(belay_rob, prediction_tib)\n\n\nbelay_glm |&gt; \n  check_model()  \n\nbelay_glm |&gt; \n  performance()  \n\nbelay_glm |&gt; \n  test_wald() \n\n\nbelay_glm |&gt; \n  parameters() |&gt; \n  display()\n\n\nbelay_tib |&gt; \n  ggplot2::ggplot(data = _, aes(x = factor(assisted_break), y = reaction_time)) +\n  geom_point(alpha = 0.7, colour = \"yellowgreen\", position = position_jitter(width = 0.1)) + \n  stat_summary(fun.data = \"mean_cl_normal\") + \n  coord_cartesian(ylim = c(0, 1030.5)) + \n  labs(x = \"Did the belayer used an assisted breaking device\", y = \"Reaction time (ms) to break the fall\") + \n  theme_light()",
    "crumbs": [
      "Generalised Linear Models"
    ]
  },
  {
    "objectID": "01_gzlms.html#footnotes",
    "href": "01_gzlms.html#footnotes",
    "title": "Generalised Linear Models",
    "section": "",
    "text": "A “belayer” is the person on the ground, managing the rope and ensuring the climber on the wall is safe.↩︎\nUnhelpfully, the ML abbreviation is also reserved for “Machine Learning”, so just be aware of this if you’re using any external resources.↩︎\nYou could substitute the term “expected” with “predicted”. The only difference is that we tend to use “predicted” when talking about a single value, and “expected” when talking about a whole distribution, but for all intents and purposes, they are the same.↩︎\nThis is not the same as the t statistic that we see in the tables.↩︎",
    "crumbs": [
      "Generalised Linear Models"
    ]
  }
]